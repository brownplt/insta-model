\documentclass[english,cleveref,submission]{programming}

%% Thesis:
%%  1. Static Python is sound, fast, and practical
%%  2. SP has unique features and restrictions for speed
%%  3. Gradual soundness is a feature: start with weak types and eventually migrate to checked types

% > Here’s the thing: I think this is the sort of thing an academic might say “oh,
% > this is too much work, it’d never work” But in fact it looks like they seem to
% > be making it work -- Shriram 2021-09-30

%% TODO before submission
%% - look out for PROGRESSIVE TYPES~\cite{pqk-onward-2012}, where changing annotations
%%   helps the static checker find new errors. ChkDict is one example.
%% - choose words: static vs typed, nonstatic vs dynamic vs untyped
%%   trying to avoid nonstatic? staticifying?
%% - use ChkDict not ChkList ... avoid confusion
%%   ... or at least mention both, be clear the focus is Dict b/c older
%% - do the LESSONS from SP come across clearly? beware making this a "progress report"
%%   - emphasize the PAYOFF from modeling early on, don't bury in sec 3
%% - terms: sound generic / concrete ... aren't people going to forget concrete? better keep as last resort
%% - perhaps a compatibility story, programmers can work on Python 3.9 until Cinder catches up b/c source code is the same
%% - "we" = all authors, NOT JUST BROWN!!!

%% TODO after submission, before camera-ready
%% - 

\newcommand{\shorturl}[2]{\href{#1#2}{#2}}
\newcommand{\SP}{Static Python}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\bcinst}[1]{\code{#1}}
\newcommand{\defeq}{=}
\newcommand{\mfeq}{=}
\newcommand{\langmid}{\mid} %% {\mathrel{\mathbf{\Big\vert}}}
\newenvironment{langarray}{\(\def\arraystretch{1.5}\begin{array}{l@{\hspace{2mm}}c@{\hspace{2mm}}l}}{\end{array}\)}
\newcommand{\ipara}[1]{\emph{#1}\/.}
\newcommand{\numbenchmark}{2}

\newcommand{\spapp}[2]{#1\,(#2)}
\newcommand{\spann}[2]{#1\!:\!#2}
\newcommand{\bmname}[1]{\textbf{#1}}
\newcommand{\typefont}[1]{\mathsf{#1}}
\newcommand{\codefont}[1]{\emph{#1}}
\newcommand{\paramtype}[2]{#1[#2]}
\newcommand{\sptype}{\typefont{T}}
\newcommand{\spteval}{\typefont{S}}
\newcommand{\sptclass}{\typefont{C}}
\newcommand{\sptX}{\typefont{X}} %% undefined type for an early example
\newcommand{\sptint}{\typefont{Int}}
\newcommand{\sptstr}{\typefont{Str}}
\newcommand{\sptbool}{\typefont{Bool}}
\newcommand{\sptfloat}{\typefont{Float}}
\newcommand{\sptflout}{\typefont{Flout}}
\newcommand{\sptdyn}{\typefont{Dyn}}
\newcommand{\sptobject}{\typefont{Object}}
\newcommand{\sptnone}{\typefont{None}}
\newcommand{\sptinstanceof}[1]{\paramtype{\typefont{Instance}}{#1}}
\newcommand{\sptoptional}[1]{\paramtype{\typefont{Optional}}{#1}}
\newcommand{\sptset}[1]{\paramtype{\typefont{Set}}{#1}}
\newcommand{\sptunion}[1]{\paramtype{\typefont{Union}}{#1}}
\newcommand{\sptrawpydict}{\typefont{PyDict}}
\newcommand{\sptrawchkdict}{\typefont{ChkDict}} %% not a real SP type, but useful for TeX
\newcommand{\sptpydict}[2]{\paramtype{\sptrawpydict}{#1, #2}}
\newcommand{\sptchkdict}[2]{\paramtype{\typefont{ChkDict}}{#1, #2}}
\newcommand{\sptenv}{\typefont{Env}}
\newcommand{\sptenvnil}{\cdot}
\newcommand{\sptenvempty}{\sptenvnil}
\newcommand{\sptvardef}[2]{\spann{#1}{#2}}
\newcommand{\sptfundef}[3]{\spapp{#1}{#2}\code{ -> }#3}
\newcommand{\sptclassdef}[4]{\mathrm{class}~\spapp{#1}{#2}:~#3;~#4}
\newcommand{\trule}[1]{\textsc{#1}}

\newcommand{\spx}{\code{x}}
\newcommand{\spf}{\code{f}}
\newcommand{\spc}{\code{c}}
\newcommand{\spprog}{\codefont{prog}}
\newcommand{\spstmt}{\codefont{stmt}}
\newcommand{\spexpr}{\codefont{expr}}
\newcommand{\spvalue}{\codefont{val}}
\newcommand{\vdashsub}[1]{\vdash_{#1}}
\newcommand{\wtprog}{\vdashsub{\mathbf{P}}}
\newcommand{\wtexpr}{\vdashsub{\mathbf{E}}}
\newcommand{\spsubt}{\mathrel{<:}}
\newcommand{\spsubteq}{\mathrel{\leq:}}
\newcommand{\spcompat}{\sqsubseteq}
\newcommand{\spconsist}{\spcompat}
\newcommand{\spmatr}{\prec}
\newcommand{\spvardef}[3]{\sptvardef{#1}{#2} = #3}
\newcommand{\spfundef}[4]{\mathrm{def}~\spapp{#1}{#2}\code{ -> }#3: #4}
\newcommand{\spclassdef}[4]{\mathrm{class}~\spapp{#1}{#2}:~#3;~#4}
\newcommand{\spself}{\code{self}}
\newcommand{\spobject}{\code{object}}
\newcommand{\spnone}{\code{none}}
\newcommand{\spint}{\codefont{int}}
\newcommand{\spbool}{\codefont{bool}}
\newcommand{\spfloat}{\codefont{float}}
\newcommand{\sppydict}[1]{\code{\{}#1\code{\}}}
\newcommand{\spchkdict}[3]{\spapp{\paramtype{\code{chkdict}}{#1, #2}}{#3}}
\newcommand{\spenvapp}[2]{\spapp{#1}{#2}}
\newcommand{\spdictref}[2]{#1[#2]}
\newcommand{\spdictset}[3]{\spdictref{#1}{#2} = #3}
\newcommand{\spobjref}[2]{#1.#2}
\newcommand{\spobjset}[3]{\spobjref{#1}{#2} = #3}
\newcommand{\spobjapp}[3]{\spobjref{#1}{\spapp{#2}{#3}}}

\newcommand{\sprred}{\rightarrow^*}

\newcommand{\mfapply}[2]{#1\,(#2)}
\newcommand{\mffont}[1]{\mathit{#1}}
\newcommand{\mftypeF}[1]{\mfapply{\mffont{F}}{#1}}
\newcommand{\mfcast}[2]{\mfapply{\mffont{Cast}}{#1, #2}}
\newcommand{\mfopt}[1]{\mfapply{\mffont{opt}}{#1}}

\newcommand{\sperror}{\mathrm{Error}}

%\overfullrule=1mm

%\usepackage{draftwatermark}
%\SetWatermarkText{DRAFT}
%\SetWatermarkScale{1}

%% BEGIN tobias pape 2021-11-06
\makeatletter
\newcommand*\abstractpart[1]{\leavevmode\unskip\par\noindent{\firamedium\color{P@GrayFG}{#1}}\enspace}
\makeatother
%% END

\usepackage[framemethod=TikZ]{mdframed}
\newmdenv[roundcorner=2pt,linewidth=0.4pt,linecolor=Black]{spcodebox}
\usepackage{amsthm}
\usepackage{mathpartir}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\usepackage[backend=biber]{biblatex}
\addbibresource{bg.bib}

\begin{document}

\title{Gradual Soundness: Lessons from Static Python}
%% ... types make things fast?
%% ... gradual migratory progressive soundness

%\titlerunning{short title}

\author[a]{Kuang-Chen Lu}
\authorinfo{(\email{LuKuangchen1024@gmail.com}) is a PhD student at Brown University.}
\affiliation[a]{Brown University, Providence, RI, USA}
\author[a]{Ben Greenman}
\authorinfo{(\email{benjamin.l.greenman@gmail.com}) is a PLT member and a postdoc at Brown University.}
\author[b]{Carl Meyer}
\authorinfo{(\email{carljm@fb.com}) FILL}
\affiliation[b]{Meta, Menlo Park, CA, USA}
\author[b]{Dino Viehland}
\authorinfo{(\email{dinoviehland@fb.com}) FILL}
\author[a]{Shriram Krishnamurthi}
\authorinfo{(\email{shriram@brown.edu}) is the Vice President of Programming Languages (no, not really) at Brown University.}

%\authorrunning{K-C. Lu, B. Greenman, C. Meyer, D. Viehland, S. Krishnamurthi}

\keywords{gradual typing, migratory typing}

\begin{CCSXML}
\end{CCSXML}
% \ccsdesc[100]{FILL}
%TODO mandatory: Please choose ACM 2012 classifications from https://dl.acm.org/ccs/ccs_flat.cfm 

\paperdetails{
  %% perspective options are: art, sciencetheoretical, scienceempirical, engineering.
  perspective=scienceempirical,
  %% State one or more areas, separated by a comma. (see 2.2)
  %% Please see list of areas in http://programming-journal.org/cfp/
  %% The list is open-ended, so use other areas if yours is/are not listed.
  area={General-purpose programming},
  %% License options include: cc-by (default), cc-by-nc
  % license=cc-by,
}

\maketitle

\begin{abstract}
  \let\paragraph\abstractpart

  \vglue10pt

  \paragraph{Context}
  % What is the broad context of the work? What is
  % the importance of the general research area?
  Gradually-typed languages allow typed and untyped code to interoperate,
  but typically come with significant drawbacks.
  In some languages, the types are unreliable;
  in others, communication across type boundaries can be extremely expensive;
  and still others allow only limited forms of interoperability.
  The research community is actively seeking a sound, fast, and expressive
  approach to gradual typing.

  \vglue10pt

  \paragraph{Inquiry}
  % What problem or question does the paper
  % address? How has this problem or question been
  % addressed by others (if at all)?
  This paper describes \SP{}, a language developed by engineers at Instagram
  that has proven itself sound, fast, and reasonably expressive in production.
  \SP{}'s approach to gradual types is essentially a programmer-tunable combination of
  the \emph{concrete}\/ and \emph{transient}\/ approaches from the literature.
  Concrete types provide full soundness and low performance overhead, but impose
  nonlocal constraints.
  Transient types are sound in a shallow sense and easier to use; they help
  to bridge the gap between untyped code and typed concrete code.

  \vglue10pt

  \paragraph{Approach}
  % What was done that unveiled new knowledge?
  We evaluate the language in its current state
  and develop a model that captures the essence of its
  approach to gradual types.
  We draw upon personal communication, bug reports, and the \SP{}
  regression test suite to develop this model.

  \vglue10pt

  \paragraph{Knowledge}
  % What new facts were uncovered? If the
  % research was not results oriented, what new
  % capabilities are enabled by the work?
  Our main finding is that the \emph{gradual soundness}\/ that
  arises from a mix of concrete and transient types is an effective
  way to lower the maintenance cost of the concrete approach.
  We also find that method-based JIT technology can eliminate the
  costs of the transient approach.
  On a more technical level, this paper contributes two artifacts:
  a model of \SP{} that passes hundreds of tests from the \SP{} codebase,
  and a performance evaluation of \SP{}. The process of formalization
  found several errors in the implementation, including fatal errors.

  \vglue10pt

  \paragraph{Grounding}
  % What argument, feasibility proof, artifacts,
  % or results and evaluation support this work?
  Our model of \SP{} is implemented in PLT Redex and tested against
  the \SP{} regression suite.
  This paper includes a small core of the model to convey the main ideas
  of the \SP{} approach and its soundness.
  Our performance claims are based on production experience in the Instagram
  web server.
  Thanks to \SP{}, global CPU usage has improved by almost 4\% within a year.

  \vglue10pt

  \paragraph{Importance}
  % Why does this work matter?
  \SP{} is the first sound gradual language whose piece-meal application
  to a realistic codebase has consistently improved performance.
  %% Prior work on Nom and Dart 2 showed that concrete types are promising.
  Other language designers may wish to replicate its approach,
  especially those who currently maintain unsound gradual languages and are
  seeking a path to soundness.

\end{abstract}


\section{Introduction}
%% alt title:
%%  {A Staged Gradual Language}
%%  {Typing for Performance}
\label{s:intro}

Gradual typing has attracted significant interest as a solution to
the debate between static and dynamic typing.
The premise is simple: let programmers introduce types in part of a
codebase and while leaving the rest untyped.
Run-time checks enforce the critical assumptions that typed code makes about
untyped components, thereby ensuring that the types are sound no matter how
untyped code behaves.

Unfortunately, the high run-time cost of sound types has split
the gradual typing community.
Industry teams have developed innovative type systems that accommodate
untyped designs, but are unsound [CITE].
These systems intentionally check nothing at run-time when untyped values enter
typed code.
Academic teams have primarily focused on the theory of sound
gradual types, formulating correctness properties and studying ever-more-descriptive types [CITE].
A few academics have studied the cost of run-time checks
in detail [CITE] and proposed implementation methods [CITE],
compiler technology [CITE],
and even weakened semantics [CITE], but these efforts have not yet
decisively closed the performance gap.
The most promising attempt is the \emph{concrete} semantics
for gradual types [CITE kafka nom etc], but it imposes nonlocal
constraints on untyped code.
In particular, concrete-typed client code is incompatible with
values created by untyped code.
Rather than search for a coordinated set of edits to make concrete
types work, developers may prefer a whole-sale migration to
a typed language.\footnote{Successful migrations of untyped codebases
to a typed language are rare, but not unheard of.
Twitter ported its server-side code from Ruby to Scala~\cite{twitter-scala}
and Dropbox moved its core sync engine from Python mypy to Rust~\cite{dropbox-rust}.}

In short, academic researchers are working to close the performance gap
without overly restricting the promise of gradual typing.
Industry researchers are sidestepping the problem with unsound types---for the most part.

This paper reports an exception to the rule among industry-made gradual type systems.
The \SP{} team at Instagram has developed a \emph{sound} type system for a subset of
Python along with a runtime system that uses soundness to drive optimizations.
The language is staged to let programmers choose between easy migrations and
full-strength optimizations, a design that we call \emph{gradual soundness}.
To a first approximation, there are three main stages:
\begin{enumerate}
  \item
    \SP{} contains a full language of \emph{shallow types}\/ that describe idiomatic
    Python code at a coarse granularity.
    Adapting an untyped module to use shallow types requires at most a few local code
    changes.
  \item
    A small set of \emph{concrete types}\/ describe generic data structures
    that guarantee the types of their elements.
    If programmers modify their code to build concrete structures instead of Python
    ones (a potentially nonlocal change), then \SP{} can speed up reads from these
    structures.
  \item
    Additional types and flags to enable further optimizations.
    These include \emph{primitive types}\/ that enable unboxed arithmetic
    and \emph{strict module semantics}\/ that makes modules immutable.
\end{enumerate}

Over the past year, Instagram has been applying gradual soundness to its
primary web server monolith.
The results are very positive.
Although only a handful of modules use \SP{} types (hundreds among thousands),
and only a few critical modules rely on concrete types and primitive types (dozens),
global CPU usage has improved by almost 4\% according to internal profiling tools.
Type-directed optimizations outweigh the cost of enforcing soundness.
Consequently, we believe the details of the \SP{} approach are of interest to
the gradual typing community at large.


\paragraph*{Contributions}
This paper makes two contributions:

\begin{itemize}
  \item
    \emph{Evidence}.
    We present evidence that \SP{} improves performance with few code changes.
    At Instagram, the web server has become significantly more efficient
    after the application of \SP{} to high-profile modules.
    Because the server code is closed-source, we additionally present data for
    \numbenchmark{} public benchmarks.
    Using only shallow types, \SP{} runs on-par with Python despite the costs of
    enforcing soundness.
    With fine-tuned types, \SP{} consistently out-performs the Python baseline.

  \item
    \emph{Mechanization}.
    To validate the soundness of \SP{}, we mechanized a core language in PLT Redex
    and ran over 200 tests adapted from the \SP{} regression suite.
    The model brings across the essence of \SP{} and lends support to the claim
    that core \SP{} is indeed sound.
    Perhaps more importantly, the \emph{process} of modeling revealed 20 significant
    issues in \SP{}.
    Four of these were critical soundness bugs.

\end{itemize}

\paragraph*{Outline}

This paper begins with an informal description of \SP{} in two parts.
First, we present a user-oriented summary of the language~(\cref{s:tour}).
Second, we present the key ingredients of the runtime system, Cinder,
that supports \SP{}~(\cref{s:runtime}).
\Cref{s:model} uses a small formal model to introduce our Redex mechanization
and to convince readers that \SP{} is based on a sound core.
\Cref{s:impl} notes important aspects of \SP{} that are not covered by the model.
\Cref{s:eval} evaluates \SP{}; it reports our experience with the language in
production and on public microbenchmarks.
The paper concludes with related work~(\cref{s:related}),
future work~(\cref{s:future}), and a final summary~(\cref{s:conclusion}).


\paragraph*{Significance}

We have written this paper with two audiences in mind.
First, we want to encourage system-builders to reproduce the
\SP{} language design.
In particular, the maintainers of optionally-typed languages
may wish to focus on shallow types and JIT compilation as a
first step toward sound gradual types.
Second, we want to entice researchers.
The \SP{} type system has many restrictions that are suprising
at first glance; for example, functions are supported only by
shallow types.
Some of these restrictions might be lifted by future research.
Others might be useful to adopt in new language designs.


\section{A Tour of \SP{}}
\label{s:tour}
%% purpose = SP looks like Pyre, adds soundness to certain types

% {Language Pipeline}
% \begin{enumerate}
%   \item Pyre as an optional, but recommended pre-check. (Not necessary for the model.)
%   \item SP type checker
%   \item Custom bytecode, Cinder JIT
% \end{enumerate}

\SP{} is part of a large codebase that includes a type system, a tailored
bytecode, and a method-based JIT compiler.
In essence, \SP{} is a type system for a new language.
The interface that it offers to programmers, however, seeks to replicate the
Python experience.
\SP{} runs untyped Python 3.8 programs with minimal changes to their behavior,
and it compiles code on-the-fly to give the same workflow and enable the same
developer tools as standard Python.
The advanced features of the \SP{} type system are offered on an opt-in basis
and arranged so that programmers can begin adding types one module at a time.

%% TODO clean up arrow between subfigs
\begin{figure}
  \begin{subfigure}[t]{0.15\columnwidth}\noindent\begin{lstlisting}

def f(x):
  return x["A"]

f({"A": 1})
  \end{lstlisting}
\end{subfigure}\(~~\rightarrow~~\)\begin{subfigure}[t]{0.32\columnwidth}\noindent\begin{lstlisting}
from __static__ import PyDict
def f(x: PyDict[Str, Int]):
  return x["A"]

f({"A": 1})
  \end{lstlisting}
  \end{subfigure}\(~~\rightarrow~~\)\begin{subfigure}[t]{0.34\columnwidth}\noindent\begin{lstlisting}
from __static__ import ChkDict
def f(x: ChkDict[Str, Int]):
  return x["A"]

f(chkdict[Str, Int]({"A": 1}))
  \end{lstlisting}
  \end{subfigure}

  \caption{A first \SP{} program and two migrations}
  \label{fig:sp-example}
\end{figure}

\Cref{fig:sp-example} presents a first example program and two modified versions that
utilize \SP{} types.
The basic program defines a function \code{f} and calls it with a dictionary
value.
\SP{} can run this program as-is and even JIT-compile the function.
The other two versions import dictionary types from the \SP{} standard library:
\begin{itemize}
  \item
    The $\sptrawpydict$ type describes normal Python dictionaries in a \emph{shallow} sense.
    At compile-time, \SP{} uses this type to check the program for logical errors.
    At run-time, \SP{} enforces the type by checking that all inputs to \code{f} are
    dictionary values.
    These checks enable optimizations within the function body.

  \item
    The $\sptrawchkdict$ type describes a \emph{concrete} dictionary data structure provided by \SP{}.
    Unlike Python dictionaries, these checked dictionaries are guaranteed to
    contain well-typed keys and values even if they escape to untyped code.
    As the body of \code{f} illustrates, the syntax for using a checked dict is standard.
    Creating a checked dict, however, requires a parameterized constructor call.
\end{itemize}
These typed versions demonstrate the multi-level nature of the \SP{} interface.
At one level, there are types that describe standard Python values that can be added
to a program with little-to-no code changes.
Beyond that, \SP{} offers special-purpose types with stronger guarantees that impose
nonlocal maintenance costs (i.e., on constructor calls).


\subsection{Type System Context and Design Goals}

The \SP{} type system is a unique synthesis of ideas from the gradual typing literature
and prior work on types for Python.
Nom~\cite{mt-oopsla-2017,mt-oopsla-2021} and PEP 484~[CITE] are two notable sources.
In addition, \SP{} is strongly influenced by the following engineering constraints:
\begin{itemize}
  \item
    Performance is the bottom line.
    At the end of the day, \SP{} needs to make code run faster in production.
  \item
    Critical code often relies on first-order functions and objects,
    at least in the Instagram web server.
    Precise types for higher-order functions and first-class classes
    are thus a low priority.
  \item
    The codebase in which \SP{} is being applied makes heavy use of Pyre [CITE]
    type annotations.
    \SP{} needs to be compatible with Pyre syntax to reduce the adoption burden.
    % On the flip side, the existence of Pyre lets \SP{} focus on a small language
    % of types that enable optimizations.
    % (\SP{} defers to Pyre the task of statically checking the full Python language.)
  \item
    A module-level granularity is acceptable.
    Once \SP{} is enabled for a module, it compiles all code in that module
    including expressions that have the dynamic type~(\cref{s:type-dynamic}).
    If this behavior is a problem for certain expressions, it is easy enough to move
    them to an untyped Python module.
\end{itemize}
%
In general, the types that \SP{} implements all enable significant optimizations
and can be validated with constant-time dynamic checks.
Types that do not meet these criteria are deferred to Pyre, even if they are widely-used.

At present, \SP{} has no plans to support all Pyre types.
The goal is to implement a small language of types efficiently.
After all, programmers can always lint their code with Pyre before running
\SP{}.

The core supported types describe basic data (integers, booleans, strings),
data structures (lists, dicts, promises),
and nominal classes.
Union types are well-supported statically and narrow down in accordance
with type tests (a form of occurrence typing~\cite{tf-icfp-2010,gsk-esop-2011}),
but are, for the most part, not supported at run-time.
Only binary unions of a type and the Python \code{None} value get enforced with a run-time check
($\sptunion{\sptype, \sptnone}$, or $\sptoptional{\sptype}$).

Three other unsupported types bear special mention:
first-class class and object types~(\cref{s:inheritance});
first-class function (callable) types; and
recursive types.
Pyre has some support for the first two, but not for recursive types.
In any event, the demand for all three types is low in our applications
of \SP{}.
%% mypy = https://github.com/python/mypy/issues/731
%% nothing in pep 484 = https://www.python.org/dev/peps/pep-0484/


\subsection{Type Dynamic}
\label{s:type-dynamic}

%% 2021-12-14 TODO:
%% - anything to say about Any being a "meta" type in Python typing module?
%%   2022-01-10 bg: not sure what the consequences of "meta type" are

Like most gradual languages, \SP{} includes a dynamic type
that allows untyped expressions within a statically-typed context.
Whenever an expression or variable lacks a type annotation, \SP{} uses the
dynamic type as a default and skips most \emph{but not all}\/ static checks.
Programmers can thus add annotations to part of a module
while leaving the rest unannotated.

In addition to serving as a default, the dynamic type also replaces any undefined types.
For example, suppose that the type $\sptflout{}$ is undefined in \SP{}
(i.e. it is a misspelling of $\sptfloat$) and that a
program contains the annotation $\sptchkdict{\sptstr}{\sptflout}$, which describes a
dictionary with integer keys and $\sptflout$-typed values.
\SP{} interprets this annotation as $\sptchkdict{\sptint}{\sptdyn}$ and allows any sort
of values inside the dictionary at runtime.

At first glance, this behavior might seem odd: why not reject undefined types with an error?
The reason is to reduce friction with existing Pyre annotations.
Pyre is a mature static type system that can check more types than \SP{} knows how to enforce
efficiently, e.g., the type $\sptset{\sptint}$ for sets of integers.
By replacing such types with dynamic, \SP{} can accept Pyre code that it does not yet fully understand.

The \SP{} dynamic type is also different than the flexible dynamic type provided
by gradual languages that satisfy the gradual
guarantees~\cite{svcb-snapl-2015}.
In \SP{}, replacing part of a type with dynamic can lead to a static error when
the compiler analyzes that part of the module.
Similarly, the addition of a dynamic type can lead to a run-time error because of the
strict checks that \SP{} applies to generic types.
\Cref{fig:gg-failure} presents two examples, one for each kind of failure:
%% \footnote{Technically, \SP{} does not even
%% have syntax for the dynamic type. The idiomatic way to ask for dynamic is to
%% omit a type annotation.}
%% \footnote{We
%% assume a standard type precision relation. Of course, one could argue that these programs
%% do not break the gradual guarantees for a type precision relation that is tailored to \SP{}.}
%%   %% tailored = describes exactly the migrations that SP allows
%% \footnote{In addition, C types~(\cref{s:c-types}) are incompatible with type dynamic.}
\begin{itemize}
  \item
    \Cref{f:gg-failure-stat} presents a fully-typed class and a partially-typed subclass.
    The subclass definition raises a compile-time error because it attempts to override
    the typed \code{m} method to return the dynamic type.

  \item
    \Cref{f:gg-failure-dyn} sends a checked dictionary with integer values to a
    function that expects dictionaries with dynamic values.
    At runtime, the function rejects this argument because its type is not an
    exact match.
\end{itemize}
These behaviors enable efficient run-time checks and additional type-directed
optimizations.
Part of the reason \SP{} weighs these benefits more heavily than the costs
is that programmers have to opt-in to a feature in order to risk the errors.
The static error above comes only after enabling the \SP{} compiler on a module.
The dynamic error comes only after adopting a concrete type (for checked dictionaries).

More to the point, \SP{} has little interest in guarantees about
\emph{removing an arbitrary annotation}.
It is, however, very interested in making sure that \emph{an untyped
module compiles with minimal code changes}.

\begin{figure}
  \begin{subfigure}[t]{0.48\columnwidth}
    \begin{lstlisting}
class A:
  def m(self) -> Int:
    return 0

class B(A):
  def m(self):
    # Error: Dyn cannot override Int
    return 0
  \end{lstlisting}
    \caption{SGG Violation: removing a type in the subclass \code{B} raises a static error}
    \label{f:gg-failure-stat}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.48\columnwidth}
  \begin{lstlisting}
from __static__ import ChkDict

def f(x: ChkDict[Str, Dyn]):
  return x["A"]

d = ChkDict[Str, Int]({"A": 1})
f(d)
# Error: f expected ChkDict[Str, Dyn]
  \end{lstlisting}
    \caption{DGG Violation: removing part of the type for the parameter \code{x} raises a dynamic error}
    \label{f:gg-failure-dyn}
  \end{subfigure}
  \caption{\SP{} ensures neither the static (SGG) nor the dynamic (DGG) gradual guarantees}
  \label{fig:gg-failure}
\end{figure}

%% TODO carl 2021-12-24 ... very important to introduce machine types with ChkDict ... emphasizes that the GG is not necessary, but only nice-to-have
%% > One area where we definitely violate the gradual guarantee and always will
%% > is primitive machine types. Unlike Python object types these do not flow to
%% > dynamic, and dynamic can never flow to them! Effectively they form a
%% > fully-typed subset of the type system that does not participate in gradual
%% > typing and can never occur in non-static code. So this violates GG because
%% > there are many examples where removing an annotation of a primitive machine
%% > type can cause static code to no longer type check.


\subsection{Concrete Types and Shallow Types}
\label{s:checked-type}

Every \SP{} type corresponds to a constant-time dynamic check.
When an unknown value reaches a boundary to typed code at run-time,
\SP{} uses these checks to quickly decide whether or not the value
matches the type (think: \code{isinstance} checks).
Unlike structural gradual type systems, \SP{} does not traverse
incoming values nor does it create wrappers to enforce behavior.

These dynamic checks try to faithfully enforce a full type, but some are incomplete---especially
for generic types.
We call the incomplete ones \emph{shallow} types.
The \SP{} strategy for fully-sound generics is to package each value with a type
tag; following the \textsf{StrongScript} authors~\cite{rzv-ecoop-2015}, we call
these \emph{concrete} types.

First off, for a basic nominally-typed language (think Java 1.4, before
generics), constant-time checks are easy to implement.
Every type has a name and every value can carry a tag that corresponds to the
name of its type.

Parameterized types (generics) are more difficult to enforce.
Consider a dictionary datatype \code{Dict[K,V]} that is parameterized by types
for its keys and values.
If a function expects a dict from integers to integers (\code{Dict[Int,Int]}),
then merely checking for a dict instance is not enough to decide whether an
incoming value matches the type.
For example, the incoming value may have been created by untyped code.

The \SP{} solution is to provide two incompatible versions of every parameterized type,
both of which support efficient run-time checks.\footnote{Currently, only a handful of built-in data structures provide shallow
and concrete versions. User-defined classes will soon follow this approach.}
For dictionaries, \SP{} provides the types $\sptrawpydict$ and $\sptrawchkdict$:
\begin{enumerate}
  \item
    \emph{Shallow type $\sptrawpydict$}.
    The default type $\sptrawpydict$ gets enforced with a constant-time check
    for dict values that ignores the elements inside a dict.
    Consequently, a parameterized static type such as $\sptpydict{\sptint}{\sptint}$
    is merely a promise of dict values at run-time; it does not promise anything about
    dict keys and values.
  \item
    \emph{Concrete type $\sptchkdict{\sptype}{\sptype}$}.
    \SP{} introduces a parameterized type for concrete, checked dictionaries
    along with a value constructor that records its type constraints.
    \cref{s:chkdict-impl} discusses the implementation further.
\end{enumerate}
%
These options let programmers decide whether to edit their code to support concrete types
or leave it as-is with shallow types.
Often, the shallow types are more attractive because concrete types impose
nonlocal changes.
If one type changes to concrete, then all data constructors that reach this type need
to change \emph{and} all their clients need to use the concrete type as well.

The upside of concrete types is that they enforce stronger type constraints.
These constraints can catch bugs and usually lead to more-efficient code.
Faster performance is not guaranteed, though, because it depends on how the dict
gets used.
\Cref{t:shallow-vs-concrete} presents a detailed comparison.
Shallow types pay a constant-time cost whenever an untyped value enters typed code
and whenever a typed context reads from a shallow value.\footnote{The predicted
costs for reads and writes assume that all \SP{} types can be validated with a
constant-time check.}
The net cost of reads may be high.
Concrete types pay a potentially-high cost for constructors,
a constant-time cost for casts and for writes,
and zero cost for dereferences.
If a value is frequently written to and rarely read from, shallow types may actually
be faster.

\begin{table}[t]
  \caption{Worst-case costs for shallow and concrete types}
  \label{t:shallow-vs-concrete}

  \centering
  \(\begin{array}{lrrrr}
     & \mbox{Constructor} & \mbox{Cast} & \mbox{Read} & \mbox{Write} \\\hline
    \sptrawpydict                &           0 & O(1) & O(1) &     0 \\
    \sptchkdict{\sptype}{\sptype} &        O(N) & O(1) &    0 &  O(1)
  \end{array}\)
\end{table}



\subsection{Gradual Class Hierarchies}
\label{s:inheritance}

One important feature of \SP{} is that class hierarchies can
mix typed and untyped code.
An untyped class can inherit from a typed one and vice-versa,
letting programmers add types to a single class independently of its ancestors
and children.

% st-ecoop-2007 has no inheritance, object is collection of members simple (Abadi--Cardelli)

Gradual class hierarchies are rarely studied in the literature, especially
for a language where truly untyped classes can enter the mix.\footnote{Two
languages that provide separate hierarchies for untyped and (gradually) typed
classes are Thorn~\cite{wzlov-popl-2010} and SafeTS~\cite{rsfbv-popl-2015}.}
The closest related work, for Nom~\cite{mt-oopsla-2017}, implements
a rather flexible point in the design space.
\SP{} implements a simpler design that places a few restrictions on types and
inheritance:

\begin{enumerate}
  \item
    To benefit from types, classes must be declared on the module top level
    and may have at most one parent.
    Nested class declarations, first-class classes, and classes with
    multiple parents default to un-optimized Python behavior~(\cref{s:impl}).

  \item
    Methods cannot be overloaded.
    This restriction comes from Python.

  \item
    Methods and fields may be overridden in arbitrary ways by untyped code.
    In typed code, however, overrides must be compatible subtypes.
    For example, a method that returns a number cannot be overridden by a method
    that returns the dynamic type; \cref{f:gg-failure-stat} shows the static error
    that arises from such a static override.

    % In fact, \SP{} compiles fields using Python slots
    % declarations (\code{\_\_slots\_\_}).
    %
    % NOTE __ (dunder) methods may be overridden e.g. __getattribute__,
    % but the normal field access syntax skips any overrides (o.f)

\end{enumerate}
%
With this context in mind, \SP{} keeps track of whether each class is typed of untyped.
Each typed class can furthermore assume that if a method has a precise (non-dynamic)
type, then all typed overrides are subtypes of this type.
\SP{} can therefore optimize dispatch from typed callers to typed methods
and use checked dispatch for other combinations.
One extra step arises when an untyped class overrides a typed method.
In this case, \SP{} creates a wrapper around the overriding method to check
that it computes type-correct results.
% Because these wrappers perform a simple first-order check, they
% are supported by an efficiently implementation that cooperates with
% \SP{}
The wrappers are handled efficiently by a tailored implementation of vtables~(\cref{s:vtable}).


\subsection{Strict Modules}
\label{s:strict-mod}

%% 2022-01-12 Although the Static Python compiler can technically be
%% used apart from strict modules, and we do that in tests for faster tests, it’s
%% not sound and we don’t allow it in prod.

Python modules are mutable collections of names.
Names can be added, deleted, and updated at runtime.
\SP{} accommodates these behaviors in the same manner as standard Python, which
necessarily limits the benefits of module types.
As an alternative, \SP{} programmers may opt in to a \emph{strict module} semantics.
Strict modules are immutable, and thus enable type-based compiler optimizations.


\subsection{Progressive Primitive Types}
\label{s:c-types}

%% 2021-12-29: TODO
%% Using the term “ctypes” for these might be confusing, since the Python stdlib
%% has a “ctypes” module that isn’t related to our primitive types. We do call the
%% base type for them CType in our compiler, but when discussing them we usually
%% call them “primitive types” or “machine types” — “C types” could also work but
%% I wouldn’t combine it into one word :)

For performance-critical applications, \SP{} includes a set of primitive types
that describe booleans and sized numbers.
Example types include \code{int64}, \code{uint64}, \code{double}.
\SP{} also includes two special datatypes, \code{Array} and \code{Vector}, that store primitives.

The purpose of these types is to enable unboxed values and arithmetic at
runtime.
Consequently, their static semantics is a \emph{progressive}~\cite{pqk-onward-2012}
refinement over the semantics of basic Python types.
Primitive types cannot be implicitly cast to non-primitive types,
including the dynamic type---another violation of the static gradual guarantee.
Operations such as \code{and} (boolean conjunction) do not accept a mix of primitive and non-primitive data.
These restrictions give programmers a static separation between fast-running
primitives and standard data.

% Neither a module-level nor a closure-level variable may have a primitive
% type---because untyped code can mutate such variables.

% For now, programmers have to write and manage primitive types.
% In the future, a preprocessor might convert Python arithmetic to primitive arithmetic.

% https://github.com/facebookincubator/cinder/issues/52


\section{Runtime System Highlights}
\label{s:runtime}
%% alt title: Runtime System Highlights
%% Non-Highlights:
%% - shadow frames = JIT feature, lite array to track Python call stack

%% 2022-01-12
%% Cinder implements primitives with unboxed C values, which are much simpler and cheaper
%% than their Python counterparts.
%%
%% Cinder handles conversions at the boundaries between \SP{} code
%% and untyped code to avoid a cascade of modify-then-run refactorings, but nowhere else.
%% Within typed code, programmers must satisfy the type checker with
%% appropriate conversions.

The \SP{} runtime system, formally known as Cinder, extends
CPython 3.8 in several ways to take full advantage of static types.
Cinder includes tailored bytecode instructions, virtual method tables,
concrete datatypes, a registry of typed modules, and a JIT compiler.

These main ingredients of Cinder may be of interest to other teams seeking to
add sound gradual typing to an existing language.
For example, Node developers may wish to fork that runtime and experiment with
bytecode instructions that leverage sound static types.


\subsection{Bytecode Instructions}

Cinder is a bytecode compiler that extends the Python instruction set.
All the standard Python instructions work similarly to Python 3.8;
in other words, an untyped Python program has the same behavior in
Cinder as in the standard interpreter.
The added instructions help with one of three tasks:
expressing runtime checks,
initializing concrete-typed values,
or efficiently performing a standard action.
As an example of the third kind, the \code{FAST\_LEN} instruction quickly
computes the length of a built-in value.
\SP{} uses type information to decide where this instruction is appropriate
and applies it as an optimization.


\subsection{Virtual Method Tables}
\label{s:vtable}

Cinder adds virtual method tables (vtables) to typed classes.
These tables help to speed up method dispatch relative to Python's
dynamic lookup.
Calls to static methods that appear in statically-typed
code use the vtable to find an address for the method.
If the method is part of a final class, then the call is further optimized
to a direct function-call jump.
(Both vtable lookup and direct jumps are supported by Cinder-specific bytecode
instructions.)
Just-in-time compilation~(\cref{s:jit}) can also upgrade vtable lookups to
function calls.

The implementation of vtables happens to be built on the Python \code{vectorcall}
protocol; it is not a from-scratch development.
That being said, Cinder vtables are specialized to check untyped overrides
of typed methods against their expected return type (instead of using a layer of
indirection for the check).

%% beware: with vtables, method resolution is different than Python b/c args. get
%%   resolved before the receiver (A.m(B) goes "A->B" in Py. and "B->A" in SP)

%% > This difference in behavior isn’t so much desired as just a consequence of making
%% > `INVOKE_METHOD` optimizable. With a normal dynamic `CALL_FUNCTION`, first
%% > the callable is placed on the stack, then the arguments, then there is a
%% > `CALL_FUNCTION` (with number of args in oparg) to perform the call. This
%% > means that first the callable is resolved, then the arguments. But with
%% > `INVOKE_METHOD` we want to resolve the callable as part of the invoke
%% > itself, since this gives us opportunity to inline-cache the target of the
%% > call instead of always having to call something dynamic and unknown that’s
%% > on the stack. So that necessarily implies that first the arguments are
%% > resolved and placed on the stack, then the callable is resolved as part of
%% > the `INVOKE_METHOD`. -Carl


\subsection{Checked Data Structures}
\label{s:chkdict-impl}

The concrete versions of built-in data structures come with both a type and an
implementation.
The implementation provides the same interface as the built-in but uses a
type tag to reject certain operations.

For example, the type $\sptchkdict{K}{V}$ describes a concrete dictionary
with keys of type $K$ and values of type $V$.
The implementation has three main components:
\begin{itemize}
  \item
    First, the constructor uses a type and a Python dictionary ($\sptrawpydict$)
    to initialize a checked dictionary.
    This constructor ensures that all values of type $\sptchkdict{K}{V}$
    begin with well-typed elements.
  \item
    Second, all write operations check their input.
    Every operation that mutates or extends a checked dictionary must
    validate any untyped arguments that it receives.
  \item
    Third, the dictionary value stores a type tag to support casts from the dynamic type.
    When a checked dictionary enters typed code from an untyped context,
    the runtime checks its type tag to see whether the key and value types are an exact match.
    For example, type $\sptchkdict{K}{V}$ matches the type
    $\sptchkdict{\sptstr}{\sptdyn}$ only if $K$ is equal to $\sptstr$
    and $V$ is equal to $\sptdyn$,
\end{itemize}
%
In general, other checked datatypes have the same three components:
a constructor, checked update functions, and a tag.


\subsection{Classloader}
\label{s:classloader}

\SP{} keeps track of typed functions and typed classes with a specialized classloader.
At runtime, the classloader keeps a registry of typed objects and whether or not they
are mutable.
The registry helps the bytecode reference objects and types via their module names.
The mutability bit lets \SP{} optimize access for immutable data while soundly allowing
patches to mutable data.


\subsection{Method-at-a-Time JIT}
\label{s:jit}

Cinder includes a JIT (just-in-time) compiler for its bytecode.
At the moment, programmers enable the JIT by supplying a list of method names
to the compiler.
Any method can be JIT-compiled whether or not it uses \SP{} types,
though types are needed to enable the most-effective optimizations.

%% 2022-01-05 Carl:
%% "All the data I’ve been sharing is from a single codebase, the primary IG web
%% server monolith. The existence of JITted but not SP modules is mostly just due
%% to the fact that opting a function into the JIT is semantically transparent and
%% (unless it reveals a latent bug in the JIT) requires no code changes, so it’s
%% quite easy to do (the only tradeoff pushing us away from JITting everything is
%% just how much CPU we spend JIT-compiling and how much memory we spend storing
%% JIT-compiled code), whereas opting a module into SP requires more work so the
%% bottleneck is available developer time to do conversions."


\section{Model}
\label{s:model}
%% purpose = types, checks, and optimizations can be formalized and analyzed

%% 2022-01-06 TODO subtyping for classes C ... matters for casts at least ... just add a Gamma, follow chain up one step at a time until refl.
%% 2022-01-04 KC will look into pbt for nom properties (I don't think there's much to see there!)

%% NOTE
%% - remark: model gives errors of the same kind in SP, but not always the same
%%   message, after all its not our goal to reproduce SP exactly

%% TODO 2021-12-28
%% - while True break another example of modeling helping out
%%   it's a second implementation, helpful to find bugs

To validate the design of \SP{}, we have developed a mechanized model using PLT
Redex~\cite{kcdeffmrtf-popl-2012}.
The model encompasses two soundness claims:
%% addresses two soundness concerns?
\begin{enumerate}
  \item
    \SP{}'s overall approach to gradual interactions is sound; namely, that interactions
    with untyped code do not undermine the validity of types.
  \item
    The typed half of \SP{} is a sound type system for Python.
\end{enumerate}
This section explores the first claim in depth by way of a tiny formal model
that illustrates the \SP{} gradual interaction strategy.
%% Compared to other gradual languages, these boundaries are quite restrictive---which
%% partly explains how it is able to perform well in practice~(\cref{s:eval}).
%% Indeed the boundaries are effectively a subset of those in Nom~\cite{mt-oopsla-2021}.
The second claim is justified by our Redex development;
in particular, by its test suite.
This section describes the Redex development in broad strokes.


\subsection{Surface Syntax and Types}
%% TODO assume well-formed; no circular defs (x:t = x)

\begin{figure}[t]
  \begin{langarray}
    \spprog & \defeq &
      \spstmt \langmid \spstmt,~\spprog
  \\
    \spstmt & \defeq &
      \spvardef{\spx}{\sptype}{\spexpr} \langmid
  \\ & &
      \spfundef{\spf}{\spann{\spx}{\sptype}}{\sptype}{\spexpr} \langmid
  \\ & &
      \spclassdef{\spc}{\spc}{\spvardef{\spx}{\sptype}{\spexpr}}{\spfundef{\spf}{\spself, \spann{\spx}{\sptype}}{\sptype}{\spexpr}}
  \\
    \spexpr & \defeq &
      \spx \langmid \spnone \langmid \spint \langmid \spbool \langmid \spfloat \langmid \spobject \langmid
      \sppydict{\spx: \spexpr, \ldots} \langmid
      \spchkdict{\sptype}{\sptype}{\sppydict{\spx: \spexpr, \ldots}} \langmid
  \\ & &
      \spapp{\spx}{\spexpr} \langmid
      \spdictref{\spexpr}{\spexpr} \langmid
      \spdictset{\spexpr}{\spexpr}{\spexpr} \langmid
      \spobjref{\spc}{\spx} \langmid
      \spobjset{\spc}{\spx}{\spexpr} \langmid
      \spobjapp{\spc}{\spf}{\spexpr}
  \\
    \sptype & \defeq &
      \sptdyn \langmid
      \sptnone \langmid
      \sptint \langmid
      \sptbool \langmid
      \sptfloat \langmid
      \sptobject \langmid
      \sptclass \langmid
  \\ & &
      \sptpydict{\sptype}{\sptype} \langmid
      \sptchkdict{\sptype}{\sptype} \langmid
      \sptunion{\sptype, \ldots}
  \\
    \sptenv & \defeq &
      \sptenvnil \langmid
      \sptvardef{\spx}{\sptype},~\sptenv \langmid
      \sptfundef{\spf}{\sptype}{\sptype},~\sptenv \langmid
      \sptclassdef{\spc}{\spc}{\sptvardef{\spx}{\sptype}}{\sptfundef{\spf}{\sptclass, \sptype}{\sptype}},~\sptenv
  \\
    \spx, \spf, \spc & \defeq & \mbox{variable names}
  \end{langarray}

  \bigskip
  \mbox{Abbreviation: $\sptoptional{\sptype} \defeq \sptunion{\sptnone, \sptype}$}

  \caption{Surface Syntax and Types}
  \label{f:surface-types}
\end{figure}

The \SP{} surface syntax is the same as Python 3.8.
Programmers may write type annotations using the PEP 484 syntax;
unannotated code gets the dynamic type by default.
The available static types provide a nominal type system with first-order functions.
These surface types are meant to be descriptive.
By design, they are not fully sound.
For this reason, we omit the surface typing judgment and merely present their
declarative syntax.
%% TODO awkward

\Cref{f:surface-types} illustrates the key aspects of the \SP{} syntax and types
using a tiny grammar.
A program is a sequence of statements;
a statement defines a variable, function, or class.
These definitions may only appear on the top level
and they all require full type annotations.
Functions must have one positional argument.
Classes must declare one parent (perhaps $\spobject$), one field, and one method.
Expressions describe values and simple computations.
The basic values consist of the following: the none value, integers,
booleans (which are the integers \code{0} and \code{1}), floating point numbers, and the top object.
There are two data structures: dictionaries and checked dictionaries~(\cref{s:checked-type}).
The remaining forms express function calls, dictionary reads and writes, object field reads and writes,
and method calls.
Types $\sptype$ include the dynamic type, types for the basic values, one type $\sptclass$ for every
user-defined class, and union types.
We assume that all unions are written a concise normal form, e.g., that
$\sptunion{\sptint, \sptunion{\sptdyn, \sptclass_0}}$ would be flattened and normalized to $\sptdyn$.
There is no function type; the only way to define a higher-order function is to use the dynamic type.

%% FILL generally, no way to express higher-order cast ... either trivial C=C or impossible C=D

Relative to \SP{} and our Redex model, the formalization in \cref{f:surface-types} omits
many details of Python including class variables, imports, conditionals, and exception handlers.
These details are crucial in the context of the Redex model, which tests whether \SP{}
soundly approximates Python.
They are less important here, where our focus is on type boundaries.
None of the omitted features give substantially new ways for typed code to interact with dynamic code.


\subsection{Evaluation Types, Casts, and Typing Judgment}

\begin{figure}[t]
  \begin{langarray}
    \spteval & \defeq &
      \sptdyn \langmid
      \sptnone \langmid
      \sptint \langmid
      \sptbool \langmid
      \sptfloat \langmid
      \sptobject \langmid
      \sptclass \langmid
    \\ & &
      \sptrawpydict \langmid
      \sptchkdict{\spteval}{\spteval} \langmid
      \sptoptional{\spteval}
  \end{langarray}

  \bigskip
  \(
    \mftypeF{\sptype_0}
    \mfeq
    \left\{\begin{array}{ll}
      \sptrawpydict & \mbox{if $\sptype_0 = \sptpydict{\sptype}{\sptype}$}
    \\
      \sptchkdict{\mftypeF{\sptype_1}}{\mftypeF{\sptype_2}} & \mbox{if $\sptype_0 = \sptchkdict{\sptype_1}{\sptype_2}$}
    \\
      \sptoptional{\mftypeF{\sptype_1}} & \mbox{if $\sptype_0 = \sptoptional{\sptype_1}$}
    \\
      \sptdyn & \mbox{if $\sptype_0 = \sptunion{\sptype, \ldots}$ and $\sptype_0 \neq \sptoptional{\sptype}$}
    \\
      \sptype_0 & \mbox{otherwise}
    \end{array}\right.
  \)

  \caption{Evaluation Types, Surface-to-Evaluation Mapping}
  \label{f:surface-to-eval-types}
\end{figure}

Evaluation type $\spteval$ are a subset of the surface types.
These are the types that \SP{} promises to soundly enforce.
\Cref{f:surface-to-eval-types} presents the syntax of evaluation types
and a retraction $\mftypeF{\cdot}$ from surface types to evaluation types.
As the retraction makes clear, the evaluation types make two main changes:
\begin{enumerate}
  \item
    The parameterized type for Python dictionaries
    $\sptpydict{\sptype_0}{\sptype_1}$ gets replaced with
    a raw type $\sptrawpydict$.
    The raw type behaves the same as $\sptpydict{\sptdyn}{\sptdyn}$ would.
  \item
    Unions get replaced with the dynamic type, except for the special case
    of unions with none ($\sptoptional{\sptype_0}$).
\end{enumerate}

The \SP{} team chose these particular evaluation types because they can all
be enforced fully and efficiently with run-time casts.


\subsubsection{Casts}

\Cref{t:cast} describes the casts in detail.
Most call for a simple tag check, i.e., a Python \code{isinstance} test.
The sole exception is optional types, which require a tag check and a test for the none value.
Even checked dictionaries rely on tag checks.
A checked dict type with keys $\sptype_0$ and values $\sptype_1$ accepts only
checked dict values that were initialized with exactly the same key and value type.
As noted in~\cref{s:type-dynamic}, the exact-match rules applies even when $\sptype_0$
or $\sptype_1$ is the dynamic type.

There are two key observations to make about casts.
First, all allowed casts can run in constant time.
No cast requires traversing a data structure.
Similarly, no cast allocates a wrapper to check higher-order behaviors in a delayed fashion.
This latter property means that blame-tracking is trivial;
casts succeed or fail immediately.

Second, the strict cast for checked dictionaries makes type compatibility simple in \SP{}.
Values that are partly-dynamic are incompatible with static types.
For example, suppose that a function expects a list of numbers.
In a typical gradual language, a list of dynamic values would be a valid input to the function.
In \SP{}, a list of dynamic values is not a valid input.
Consequently, \SP{} can use a simple ahead-of-time strategy to enforce soundness.
Any position where the dynamic type meets a static type needs a cast.
Other meetings between types can be resolved statically.

\begin{table}[t]
  \caption{How to Enforce the Evaluation Types}
  \label{t:cast}

  \(\begin{array}{l@{\quad}l}
    \mbox{Eval. Type $\spteval$} & \mbox{Description of run-time check} \\\hline
    \sptoptional{\spteval_0} & \mbox{Accepts either the none value or values that match $\spteval_0$} \\
    \sptrawpydict & \mbox{Accepts any Python dictionary} \\
    \sptchkdict{\spteval_0}{\spteval_1} & \mbox{Accepts checked dictionaries parameterized by $\spteval_0$ and $\spteval_1$} \\
    %% \sptchkdict{\spteval_0}{\spteval_1} & \mbox{Accepts checked dictionaries with the same key and value types (${\spteval_0, \spteval_1}$)} \\
    \sptdyn & \mbox{No check needed, accepts any value} \\
    \sptobject & \mbox{Accepts any value (except primitives,~\cref{s:c-types})} \\
    \sptclass_0 & \mbox{Accepts instances of class $\sptclass_0$} \\
    \sptnone & \mbox{Accepts the Python none value} \\
    \sptint & \mbox{Accepts integers} \\
    \sptbool & \mbox{Accepts booleans (the integers \code{0} and \code{1})} \\
    \sptfloat & \mbox{Accepts floats}
  \end{array}\)
\end{table}


\subsubsection{Expression Typing, Cast Insertion}

To better show where \SP{} needs to insert casts, we present a selection of typing rules.
Recall that a program declares variables, functions, and classes~(\cref{f:surface-types}).
These statements come with types and fill an environment ($\sptenv$).
Relative to the current environment, the expressions within each statement must
satisfy the judgment in~\cref{f:eval-types} 

The first two rules are for function application.
A typed function may be applied an argument that matches its domain type,
in which case it computes a value that matches its codomain type.
A dynamically-typed variable may be applied to any input (that matches the top
type $\sptobject$) to yield a dynamically-typed result.
The next rules illustrate writes to Python dictionaries and checked dictionaries.
A Python dict may be updated with any kind of key and value.
By contrast, a checked dict requires keys and values that match its type parameters.
Next we have two rules for classes:
\trule{F-Set} says that writes to a class field must match the declared field
type; and \trule{M-App} shows that typed methods impose similar constraints as
typed fuctions.

The final two rules, \trule{C-Sub} and \trule{Matr}, depend on auxiliary judgments
for consistent subtyping ($\spcompat$) and materialization ($\spmatr$).
Consistent subtyping relates type $\spteval_0$ to type $\spteval_1$ if they are
related by static subtyping ($\spteval_0 \spsubteq \spteval_1$) or if
$\spteval_1$ is the dynamic type.
When this relation holds, it is safe to upcast a value from type $\spteval_0$
to type $\spteval_1$.
Materialization relates the dynamic type to any non-dynamic type.
Occurrences of the \trule{Matr} rule are downcasts that require a
run-time check.

The name \emph{materialization} and the rule \trule{Matr} are inspired by prior
work~\cite{clps-popl-2019}.
By itself, the judgment is merely an upside-down type precision relation~\cite{g-icfp-2013,sv-dls-2008}.
Combined with the typing rule, however, materialization is a concise way to find
where a well-typed program needs to insert casts to ensure soundness.
For example, suppose that $\spf$ is a function from integers to the dynamic
type and that $\spx$ is a variable with the dynamic type.
An application $\spapp{\spf}{\spx}$ can satisfy type $\sptfloat$ using two materializations:

\medskip
\begin{mathpar}
  \inferrule*[left=Matr]{
    \inferrule*{
      \inferrule*[left=Matr]{
        \sptenv_0 \wtexpr \spx : \sptdyn
        \\
        \sptdyn \spmatr \sptint
      }{
        \sptenv_0 \wtexpr \spx : \sptint
      }
    }{
      \sptenv_0 \wtexpr \spapp{\spf}{\spx} : \sptdyn
    }
    \quad
    \sptdyn \spmatr \sptfloat
  }{
    \sptenv_0 \wtexpr \spapp{\spf}{\spx} : \sptfloat
  }

  \mbox{Where $\sptenv_0 = \sptvardef{\spx}{\sptdyn}, \sptfundef{\spf}{\sptint}{\sptdyn}$}
\end{mathpar}
\medskip
Consequently, this type derivation calls for two casts at run-time.

We end here, with materialization, rather than present a semantics
for the formalization.
After all, the main benefit of a full formal semantics is to validate
the behavior of complex expressions---a job
better left to the mechanized Redex model.

Furthermore, our casts-via-materialization rule is more aggressive
than what \SP{} actually does to insert casts because
\SP{} supports interactions with plain Python code.
Because it chooses not to recompile Python modules, \SP{} inserts
casts in an overapproximate way:
\begin{itemize}
  \item
    typed functions and methods begin by checking all their arguments;
  \item
    typed classes check all field writes;
  \item
    checked dictionaries check all writes; and
  \item
    typed code checks the results of function calls, references, etc. whenever there is a materialization from the dynamic type.
\end{itemize}
\Cref{s:impl} explains
how the type-directed optimizer avoids some
of these checks.


\begin{figure}[t]
  \fbox{$\sptenv \wtexpr \spexpr : \spteval$}~selected rules
  \begin{mathpar}
    \inferrule*[left=S-App]{
      \sptfundef{\spf_0}{\spteval_0}{\spteval_1} \in \sptenv_0
      \\\\
      \sptenv_0 \wtexpr \spexpr_0 : \spteval_0
    }{
      \sptenv_0 \wtexpr \spapp{\spf_0}{\spexpr_0} : \spteval_1
    }

    \inferrule*[left=D-App]{
      \sptvardef{\spf_0}{\sptdyn} \in \sptenv_0
      \\\\
      \sptenv_0 \wtexpr \spexpr_0 : \sptobject
    }{
      \sptenv_0 \wtexpr \spapp{\spf_0}{\spexpr_0} : \sptdyn
    }

    %% TODO show dict read rules?
%    \inferrule*[left=PD-Ref]{
%      \sptenv_0 \wtexpr \spexpr_0 : \sptrawpydict
%      \\\\
%      \sptenv_0 \wtexpr \spexpr_1 : \sptobject
%    }{
%      \sptenv_0 \wtexpr \spdictref{\spexpr_0}{\spexpr_1} : \sptdyn
%    }
%
%    \inferrule*[left=CD-Ref]{
%      \sptenv_0 \wtexpr \spexpr_0 : \sptchkdict{\spteval_0}{\spteval_1}
%      \\\\
%      \sptenv_0 \wtexpr \spexpr_1 : \spteval_0
%    }{
%      \sptenv_0 \wtexpr \spdictref{\spexpr_0}{\spexpr_1} : \spteval_1
%    }

    \inferrule*[lab=PD-Set]{
      \sptenv_0 \wtexpr \spexpr_0 : \sptrawpydict
      \\\\
      \sptenv_0 \wtexpr \spexpr_1 : \sptobject
      \\\\
      \sptenv_0 \wtexpr \spexpr_2 : \sptobject
    }{
      \sptenv_0 \wtexpr \spdictset{\spexpr_0}{\spexpr_1}{\spexpr_2} : \sptnone
    }

    \inferrule*[lab=CD-Set]{
      \sptenv_0 \wtexpr \spexpr_0 : \sptchkdict{\spteval_0}{\spteval_1}
      \\\\
      \sptenv_0 \wtexpr \spexpr_1 : \spteval_0
      \\\\
      \sptenv_0 \wtexpr \spexpr_2 : \spteval_1
    }{
      \sptenv_0 \wtexpr \spdictset{\spexpr_0}{\spexpr_1}{\spexpr_2} : \sptnone
    }

    %\inferrule*[left=F-Ref]{
    %  \spann{\spx_0}{\spteval_0} \in \spenvapp{\sptenv_0}{\spc_0}
    %}{
    %  \sptenv_0 \wtexpr \spobjref{\spc_0}{\spx_0} : \spteval_0
    %}

    \inferrule*[left=F-Set]{
      \spann{\spx_0}{\spteval_0} \in \spenvapp{\sptenv_0}{\spc_0}
      \\\\
      \sptenv_0 \wtexpr \spexpr_0 : \spteval_0
    }{
      \sptenv_0 \wtexpr \spobjset{\spc_0}{\spx_0}{\spexpr_0} : \sptnone
    }

    \inferrule*[left=M-App]{
      \sptfundef{\spf_0}{\spc_0, \spteval_0}{\spteval_1} \in \spenvapp{\sptenv_0}{\spc_0}
      \\\\
      \sptenv_0 \wtexpr \spexpr_0 : \spteval_0
    }{
      \sptenv_0 \wtexpr \spobjapp{\spc_0}{\spf_0}{\spexpr_0} : \spteval_1
    }

    \inferrule*[left=C-Sub]{
      \sptenv_0 \wtexpr \spexpr_0 : \spteval_1
      \\
      \spteval_1 \spcompat \spteval_0
    }{
      \sptenv_0 \wtexpr \spexpr_0 : \spteval_0
    }

    \inferrule*[left=Matr]{
      \sptenv_0 \wtexpr \spexpr_0 : \spteval_1
      \\
      \spteval_1 \spmatr \spteval_0
      %% side condition: result type is not a primitive
    }{
      \sptenv_0 \wtexpr \spexpr_0 : \spteval_0
    }

  \end{mathpar}

  \begin{minipage}[t]{0.5\columnwidth}
    \fbox{$\spteval \spcompat \spteval$}
    \begin{mathpar}
      \inferrule*{
        \spteval_0 \spsubt \spteval_1
      }{
        \spteval_0 \spcompat \spteval_1
      }

      \inferrule*{
      }{
        \spteval_0 \spcompat \sptdyn
      }
  \end{mathpar}

  \end{minipage}\begin{minipage}[t]{0.5\columnwidth}
    \fbox{$\spteval \spmatr \spteval$}
    \begin{mathpar}
      \inferrule*[right={$\spteval_0 \neq \sptdyn$}]{
      }{
        \sptdyn \spmatr \spteval_0
      }
    \end{mathpar}
  \end{minipage}

  \medskip
  \fbox{$\spteval \spsubteq \spteval$}~reflexive, transitive closure of the following $\spsubt$ relation:
  \begin{mathpar}
    \inferrule*{
    }{
      \spteval_0 \spsubt \sptobject
    }

    \inferrule*{
    }{
      \sptbool \spsubt \sptint
    }

    \inferrule*{
      \spteval_0 \spsubt \spteval_1
    }{
      \spteval_0 \spsubt \sptoptional{\spteval_1}
    }

%    \inferrule*{
%    }{
%      \spteval_0 \spsubt \spteval_0
%    }
%
%    \inferrule*{
%      \spteval_0 \spsubt \spteval_1
%      \\
%      \spteval_1 \spsubt \spteval_2
%    }{
%      \spteval_0 \spsubt \spteval_2
%    }
  \end{mathpar}

  \caption{Expression Typing, Consistent Subtyping, and Materialization}
  \label{f:eval-types}
\end{figure}


%\subsubsection{Program Typing, Inheritance Restriction}
%
%\Cref{f:program-typing} presents the typing judgment for programs,
%which folds over a sequence of statements to build a type environment.
%The typing rules for variables and functions are standard.
%We draw attention to the rule for classes, in particular the override condition.
%A class may override fields and methods from its parent;
%if it does, the overriding definition must be a \emph{static subtype} of the original.
%For example, a method that returns an integer can be overridden by a method that returns
%a boolean, but it cannot be overriden by a method that returns the dynamic type.
%This restriction lets the \SP{} compiler avoid some run-time checks; it can assume
%that static methods return reliable values instead of having to insert checks for subtypes.
%
%\begin{figure}[t]
%  \fbox{$\sptenv \wtprog \spprog \dashv \sptenv$}
%  \begin{mathpar}
%    \inferrule*{
%      \sptenv_0 \wtprog \spstmt_0 \dashv \sptenv_1
%      \\\\
%      \sptenv_1 \wtprog \spprog_0 \dashv \sptenv_2
%    }{
%      \sptenv_0 \wtprog \spstmt_0, \spprog_0 \dashv \sptenv_2
%    }
%
%    \inferrule*{
%      \sptenv_0 \wtexpr \spexpr_0 : \spteval_0
%    }{
%      \sptenv_0 \wtprog \spvardef{\spx_0}{\spteval_0}{\spexpr_0} \dashv \sptvardef{\spx_0}{\spteval_0},~\sptenv_0
%    }
%
%    \inferrule*{
%      \sptenv_1 = \sptfundef{\spf_0}{\spteval_0}{\spteval_1},~\sptenv_0
%      \\\\
%      \sptenv_1 \wtexpr \spexpr_0 : \spteval_1
%    }{
%      \sptenv_0 \wtprog \spfundef{\spf_0}{\spann{\spx_0}{\spteval_0}}{\spteval_1}{\spexpr_0} \dashv \sptenv_1
%    }
%
%    \inferrule*{
%      \sptenv_1 = \sptclassdef{\spc_0}{\spc_1}{\sptvardef{\spx_0}{\spteval_0}}{\sptfundef{\spf_1}{\spc_0, \spteval_1}{\spteval_2}}, \sptenv_0
%      \\\\
%      %% TODO are field overrides exact?
%      \mbox{if $\spann{\spx_0}{\spteval_0'} \in \spenvapp{\sptenv_1}{\spc_1}$ then $\spteval_0 = \spteval_0'$}
%      \\\\
%      %% TODO can T override dyn?
%      \mbox{if $\sptfundef{\spf_1}{\spc_2, \spteval_1'}{\spteval_2'} \in \spenvapp{\sptenv_1}{\spc_1}$ then $\spteval_1' \spsubteq \spteval_1$ and $\spteval_2 \spsubteq \spteval_2'$}
%      \\\\
%      \sptenv_1 \wtexpr \spvardef{\spexpr_0} : \spteval_0
%      \\
%      \spann{\spself}{\spc_0}, \spann{\spx_1}{\spteval_1}, \sptenv_1 \wtexpr \spexpr_1 : \spteval_2
%    }{
%      \sptenv_0 \wtprog \spclassdef{\spc_0}{\spc_1}{\spvardef{\spx_0}{\spteval_0}{\spexpr_0}}{\spfundef{\spf_1}{\spself, \spann{\spx_1}{\spteval_1}}{\spteval_2}{\spexpr_1}} \dashv \sptenv_1
%    }
%  \end{mathpar}
%
%  \caption{Program Typing}
%  \label{f:program-typing}
%\end{figure}


\subsection{Type Soundness}

The central question for the model is whether \SP{}'s approach to gradual typing
is sound.
No matter how an expression mixes typed and untyped code, its static
\emph{evaluation} type should be an accurate prediction about the results
of evaluation.
Using the metavariable $\spvalue$ to represent heap-allocated Python values
and $\sprred$ as the evaluation relation (which would need to maintain a heap),
an approximate soundness theorem is as follows:

\begin{theorem}[Type Soundness]
  If\ \(~\vdash \spexpr : \sptype\)
  then one of the following holds:
  \begin{itemize}
    \item
      \(\spexpr \sprred \spvalue
        \mbox{ and }
        \vdash \spvalue : \mftypeF{\sptype}
      \)
    \item
      \(\spexpr\) diverges
    \item
      \(\spexpr\) reduces to an allowed error
  \end{itemize}
\end{theorem}

As mentioned above, our argument for soundness has two parts
and the question at hand is whether the boundaries to type dynamic
are adequately protected with run-time checks.
The Redex model's comprehensive test suite provides the other half of the
argument; namely, that \SP{} is a sound type system for Python.

The direct path to boundary soundness involves two observations.
First, every evaluation type can be enforced with a decidable cast.
This is clearly the case for basic values such as floats.
It true for parameterized types such as $\sptchkdict{\spteval_0}{\spteval_1}$
because of their strict semantics.
Second, all boundaries relate the dynamic type to an evaluation type.
There are no significant boundaries that relate a partially static type
to an evaluation type.
This is due to the strict semantics for checked types and the fact that
$\sptoptional{\sptdyn}$ normalizes to the dynamic type.

Alternatively, observe that the boundaries in \SP{} are a subset
of the boundaries in Nom, a gradual language that comes with a detailed
proof of soundness~\cite{mt-oopsla-2017,mt-oopsla-2021}.
Given an \SP{} program, it is possible to derive a Nom program with
the same type boundaries by replacing every checked dict instance
with a fresh class type.
Because different checked dict instantiations are incompatible with
one another, the \SP{} boundaries impose the same constraints as the Nom
boundaries.


\subsection{Redex Development, \SP{} Conformance}
%% pbt for soundness?

By contrast to the small model language presented in this section,
our Redex model covers both the typed--untyped boundaries in \SP{}
and a large subset of Python.
Notable Python features include module imports, loops, exception handlers, and
delete statements (for class members and attributes).
To ensure that these features are implemented correctly and soundly, the
model includes hundreds of test programs adapted from the \SP{} test suite.\footnote{
  Most of the N tests are direct translations of \SP{} conformance tests to
  the syntax of the model.
  A program that implements the translation is included with our Redex development.
  The other N tests are not directly expressible in the model; for these, we
  translated salient aspects of each test by hand.}
These tests exercise a variety of Python behaviors, thereby giving us confidence
that the model is sound for Python.
Some tests intentionally raise type errors, some raise exceptions, and the rest
compute a specific result.
We have additionally verified that the model and \SP{} agree on these end-to-end
behaviors.
Thus we believe that \SP{} is sound as well.
%% ... mention that SP is a large and hard-to-analyze language ... opaque?

The model intentionally does, however, not cover all of Python.
Some aspects of \SP{} are left out because they are straightforward to
handle soundly (e.g. primitive types~\cref{s:c-types} and strict modules~\cref{s:strict-mod}).
Aspects of \SP{} that do not pertain to soundness, such as the quality of error messages,
are also omitted.
Other Python features are left out because their \SP{} semantics is identical
to the baseline Python semantics.
These features include \code{eval}, first-class classes, multiple inheritance, and module-level cells;
see \cref{s:impl} for further discussion.
Lastly, the model does not include type-directed optimizations.
Validating the correctness of optimizations is a separate task that requires
significant components; namely, a model of the \SP{} bytecode and a faithful
rendering of its type-based transformations.
Our model establishes soundness as a baseline.
Future work can use soundness to reason informally about optimizations,
or even take it as a starting point for compiler verification.


\subsection{The Payoff: Issues Reported}
%% 2022-01-11 FILL TODO reword, excitement!

While modelling \SP{}, we found several issues with the language
and submitted them as bug reports to the \SP{} developers.
Overall, we reported 25 issues~(listed in \cref{a:github-issues}).
Four of these were critical soundness issues, one of which resulted in
a segmentation fault.
All but one of these soundness issue have been fixed.
Five issues were relatively minor; these dealt with confusing error messages
and incorrect tests.
The remaining 16 issues report bugs in language design and implementation.
The \SP{} team has acknowledged these bugs as such via GitHub a label (\code{sp-correctness}).

In addition to these formal issue reports,
we had several long discussions together (between Brown and Instagram)
about finer points in the language design.



\section{Scaling to Python}
\label{s:impl}

%% TODO unions tracked statically in SP, erased to dyn unless narrowed

%% TODO SP side channels:
%% ?? eval,
%% the locals and globals dicts (the latter can be accessed through frame objects at the moment),
%% and mutable closure cells.

%% TODO built-in vs. user defined ... 
%% Carl: I don't think this is quite true the way it's phrased here? in general
%%   for SP classes we wrap non-static subclass override methods at runtime (in our
%%   vtable implementation) to enforce correct return type, so that we don't need
%%   exact types, we can trust that Liskov is not broken.
%%     The cases where we tend to be limited to exact types is in optimizing
%%   operations on builtin types, where they aren't really static types and
%%   methods called on them aren't going through SP vtables, so we don't have
%%   the vtable wrappers to enforce LSP, we just know the behavior of the exact
%%   builtin type


This section explains significant aspects of \SP{} that lie outside the
scope of our model.
These aspects include sound interactions with Python code,
the bytecode language and optimizations, and dynamic Python features
that \SP{} does not yet ascribe types to.


\subsection{Open-World Python Interactions}

Although \SP{} is technically a new language, it is designed for gradual adoption.
Python programmers should be able to add types one module at a time to an
existing codebase.
Consequently, \SP{} supports interactions with Python modules in the only viable
way: by letting the Python code run with minimal constraints.
After all, if programmers need to fix every module in a codebase before they can
run tests after typing a first module, then the gradual story is lost.

Interactions with open-world~\cite{vss-popl-2017} Python code pose a minor
threat to soundness.
Typed functions (and methods) cannot assume that their arguments are
well-typed.
Arguments from typed contexts can be validated by either static checks
or materialization casts, but arguments from untyped contexts are unchecked.
For this reason, \SP{} compiles every typed function to check its inputs.
\Cref{s:optimize} explains how the optimizer avoids some of these checks.
%% 2022-01-09 worth mentioning primitive types? they get cast like anything else
%% ... I guess the trick is that primitives don't materialize in typed code

FILL sound generics / checked types also need to guard their writes

There are two cases, however, where new \SP{} types do add constraints to
existing Python code.
First, sound generic types (e.g. $\sptchkdict{\sptype_0}{\sptype_1}$)
force value-suppliers to use a checked constructor.
Existing code can \emph{use}\/ a checked generic via the same API as on
un-checked one, but can create new values only using the \SP{} constructors.
Second, the strict setting for modules prevents updates to module-level variables.
If existing code happens to mutate a member of a newly-strict module, a run-time
error will occur.


\subsection{Bytecode and Optimizations}
\label{s:optimize}

\SP{} can generate more-efficent code than Python because it targets
the Cinder runtime.
Cinder extends Python with bytecode instructions to check types at run-time,
to construct \SP{} data structures, and to perform optimized actions.
\Cref{t:bytecode} lists a few representative bytecode instructions.

\begin{table}
  \caption{Cinder bytecode instructions}
  \label{t:bytecode}

  \begin{tabular}{lll}
    {Instruction} & {Purpose} & {Description} \\\hline
    \bcinst{CAST} & Soundness & Assert that a value matches a type \\
    \bcinst{CHECK\_ARGS} & Soundness & Cast all inputs to a function \\[1ex]

    \bcinst{BUILD\_CHECKED\_MAP} & Constructor & Make a checked dictionary \\
    %\bcinst{BUILD\_CHECKED\_LIST} & Constructor & Make a checked list \\
    \bcinst{TP\_ALLOC} & Constructor & Make a \SP{} object \\[1ex]

    \bcinst{INVOKE\_FUNCTION} & Optimization & Execute a direct function call \\
    \bcinst{INVOKE\_METHOD} & Optimization & Execute a method via the object's vtable \\
    \bcinst{LOAD\_FIELD} & Optimization & Read an object field \\
    \bcinst{STORE\_FIELD} & Optimization & Write to an object field \\
    \bcinst{FAST\_LEN} & Optimization & Get length of a built-in value \\
    \bcinst{REFINE\_TYPE} & Optimization & Type declaration for the JIT \\

%%% Other bytecodes ... either from Carl's talk or the SP repo
%CONVERT_PRIMITIVE
%INT_LOAD_CONST_OLD
%JUMP_IF_NONZERO_OR_POP
%JUMP_IF_ZERO_OR_POP
%LIST_DEL
%LOAD_ITERABLE_ARG
%LOAD_LOCAL
%LOAD_MAPPING_ARG
%POP_JUMP_IF_NONZERO
%POP_JUMP_IF_ZERO
%PRIMITIVE_BINARY_OP
%PRIMITIVE_BOX
%PRIMITIVE_COMPARE_OP
%PRIMITIVE_LOAD_CONST
%PRIMITIVE_UNARY_OP
%PRIMITIVE_UNBOX
%RETURN_PRIMITIVE
%SEQUENCE_GET
%SEQUENCE_REPEAT
%SEQUENCE_SET
%STORE_LOCAL

  \end{tabular}
\end{table}

The two instructions that express run-time checks are \code{CAST}
and \code{CHECK\_ARGS}.
The former checks a value against a type.
The latter is for functions and methods; it checks all inputs to a function
against their declared types.

\Cref{t:bytecode} presents two instructions that allocate \SP{}-specific data structures.
The first creates a checked dictionary; the second creates an object with a vtable~(\cref{s:vtable}).
Cinder comes with a similar instruction to build checked lists.
The developers will add more instructions along the same lines in the future.

The remaining instructions are for optimization.
Both \bcinst{INVOKE\_FUNCTION} and \bcinst{INVOKE\_METHOD} are alternatives
to Python's dynamic call dispatch.
The former jumps to a statically-determined address;
the latter uses a v-table lookup to quickly find an address at run-time.
The load and store instructions for fields improve upon Python's generic attribute lookup.
In the JIT, these instructions can sometimes be further optimized to a single assembly instruction.
The \bcinst{FAST\_LEN} instruction gets the length of a built-in datatype directly, instead
of using the Python \code{\_\_len\_\_} attribute.
Lastly, the \bcinst{REFINE\_TYPE} instruction tells the JIT about the type of a local value whose
type is not clear from the context.


\paragraph{Indirect Optimization}

In addition to upgrading bytecode instructions to optimized ones, \SP{} takes care
to minimize the type checks that it executes at run-time.
In other words, the goal is to slow code down as little as possible.

Part of this goal is met by inserting casts only in positions where the dynamic
type flows into a static type.
The materialze rule in the model illustrates this policy.
Typed functions, however, are compiled with a \bcinst{CHECK\_ARGS} instruction
that casts all arguments.
By convention, this instruction always appears on the first line of a typed
function body.
The optimizer uses this convention to skip argument checks by jumping past them
when it is safe to do so; namely, whenever a typed function (or method) gets
called in a typed context.
In this way, all untyped calls get the necessary argument check and all typed
calls benefit from materialize checks at the call-site.


\subsection{Dynamic Python Features}
\label{s:dynamic-python}

%% TODO 2022-01-09 be careful here ... the model may indeed handle module vars
%%  the thing is, they're untyped and (maybe) not tested

\SP{} does not ascribe types to the following Python features.
These are not covered in our model because the implementation simply
assigns the dynamic type and lets the runtime treat them as untyped
Python code.
For each, we claim that the dynamic type is a reasonable choice;
accurate static types would be difficult to maintain.


\paragraph{First-Class Classes}

\SP{} does not attempt to type first-class classes: partly because they have
yet to appear in performance-critical code at Instagram and partly because it
is unclear how to incorporate them into the nominal type system.
The straightforward but restrictive approach is to force code that uses a
first-class class to expect subtypes of a particular named static class.
Flatt et al.~\cite{fkf-popl-1998} propose a more-flexible approach, %(specifically for mixins)
but it requires a second layer of \emph{interface types} atop the nominal hierarchy.
MonNom~\cite{mt-oopsla-2021} uses interfaces in a similar way to accommodate
structural objects.
Adding an interface layer to \SP{} is an open question.

% in particular , Python's multiple inheritance would complicate the mixin story.
% Classic mixins rely on structural types~\cite{bc-oopsla-1990}.

%Notes on first-class classes being untyped:
%- mixin-based code tends to structrural types
%- nominality overly restrictive, get forced into <: hierarchy
%- but don't have NO IDEA for wat to do
%  + classes and mixins paper shows one idea,
%    ... need complicated mixin study??? (many sub objects)
%    built new type system to be mixin-aware, layered atop java
%    unsure if such extension works for SP
%    furthermore, class-and-mixins depends on Java single inheritance
%  + bracha cook mixin pattern
%  + tate muehlboeck structural object + interfaces


\paragraph{Multiple Inheritance}

%% https://docs.python.org/3/reference/datamodel.html
%% TODO check Dino email, confirm with C&D that layout conflict is unrelated to vtables

Python allows classes to inherit from a list of parents.
When resolving a method call to such a class, Python
dynamically traverses the parent list in a fixed order
seeking a first match.

Due to the dynamic method resolution order (MRO),
\SP{} does not track types for classes with multiple parents.
Method calls to these classes execute the same way as
in Python, with no type-directed optimizations to speed up dispatch.


\paragraph{Dynamic Execution}
%% TODO is eval a side channel, or safe?
%% TODO check that eval/exec args really are unoptimized / sound etc.
Results computed by calls to \code{eval} and \code{exec}
have the dynamic type.
Their inputs run without being rewritten
by the optimizer.

Studies of JavaScript and R have shown that many uses of dynamic execution
can be removed through simple adjustments~\cite{rhbv-ecoop-2011,gdkkv-oopsla-2021,mrmv-esop-2012}.
Assuming these findings carry over to Python, we recommend either similar
adjustments or the introduction of semantically-descriptive replacements
over attempting to type eval.


\paragraph{Module-Level Variables}

Any Python module can read and write to the module-level variables of another module.
When compiling a module, \SP{} therefore assumes that its module-level variables
may be modified by untyped code and assigns the dynamic type.

Cinder offers \emph{strict modules} as an alternative to the Python
semantics.\footnote{https://instagram-engineering.com/python-at-scale-strict-modules-c0bb9245c834}
If a programmer chooses to declare a module as strict, then its module-level varibles
are immutable and thus typeable.

Another potential solution is for the Cinder runtime to check that writes to module-level variables
preserve their types.
There are two downsides to this idea:
it will add some performance overhead,
and it will force developers to rewrite untyped code in order to fix any errors that arise.
%% Because of the latter concern, type dynamic is a very reasonable default.


\paragraph{Set attribute}

Carl: Removing the use of super-dynamic Python features like
\code{\_\_setattr\_\_} that we don’t support. So far in the cases where
we’ve done this it hasn’t required a major rewrite.


\section{Production Experience}
\label{s:eval}
%% 2021-12-15: C&D may have an SP off/on switch soon to measure totals

%% 2022-01-12
%% We did have some cases where strict semantics caused a problem, but in the
%% majority of conversions it didn’t (or if it did initially, the fix was
%% trivial.) There was at least one case where I would have converted more code to
%% static but couldn’t easily do so because of strictness issues with a larger
%% framework reliant on metaclasses, and so I instead extracted the hottest code
%% paths into a new strict/static module and called them. I have a plan to go back
%% to this and improve the framework itself to be strict so more code can be
%% converted. I know of at least one other case where we did modify a framework
%% that relied on side-effecting decorators so that it could be used without the
%% side effects.

%% 2022-01-12 All SP mods are strict. Although the Static Python compiler can technically be
%% used apart from strict modules, and we do that in tests for faster tests, it’s
%% not sound and we don’t allow it in prod. The way a real module gets compiled as
%% Static Python is in fact via the strict modules import loader, which detects
%% both `import __strict__` and `import __static__` markers, treating the former
%% as strict only and the latter as both strict and static. So we don’t provide
%% any straightforward way (either in IG codebase or generally with Cinder) to
%% have a module imported as static without also being strict.

%% 2022-01-12 Looks like 3 use CheckedDict and 6 use CheckedList.
%% 2022-01-12 

%% 2022-01-11 TODO how many Checked modules in production?
%% 2022-01-11 TODO how many strict modules in production?
%% 2022-01-11 TODO talk about primitive types:
%% Carl: "I just did a very rough grep check and found that we have only 12 modules
%% currently using primitive types: all int32 and int64 and cbool. So they are
%% used pretty sparingly, but they are also definitely critical to our ability to
%% match Cython in a few of the  hottest code paths that used to be Cython."

%% 2022-01-05 TODO We are currently JITting at least some functions in 873
%% modules, but we are about to increase this by about an order of magnitude,
%% independent of Static Python.

%% 2022-01-04 TODO have no evidence that "any module" typing works out ... team
%% tried hard to get network effects ... "We explicitly aimed for converting
%% related modules like this to improve network effects and get better
%% performance."

%% 2022-01-04 TODO ask C&D how many teams have adopted SP ... get sense of adoption

% "The workload for those rough percentage numbers is “Instagram web server
%  production.” We have pretty extensive production profiling that lets us
%  identify CPU savings with good granularity when a change lands in prod, and the
%  observed CPU savings for a variety of staticification changes we’ve landed over
%  the last half sum up to 2% of total IG prod CPU usage." --Carl

%% TODO 2021-12-29 Carl
%% We definitely did find some regressions (especially when converting Cython, but
%% also one or two in non-Cython conversions.) But we were never satisfied with a
%% regression, and we were always able to apply the JIT or improve SP
%% optimizations or adjust the code to turn the regression neutral or positive.
%% (In fact I think all? the non-Cython regressions I can recall boiled down to
%% “we forgot to enable the JIT for an important function” and just rectifying
%% that took care of it.)

%% TODO think about it 2021-12-29 Ben
%% > + Is there any sense in which the 530 converted modules are a big
%% > sample? Maybe not, since as you say 530 is tiny for the codebase as a
%% > whole. But do they exercise all sorts of behavior? Do they form a big
%% > sample of certain parts of the Instagram codebase?


\begin{table}
  \caption{Overview: \SP{} in production}
  \label{t:prod-stat}
  \begin{tabular}{l@{~~}ll@{~~}l}
    8     & months in production & 3.7\% & global CPU improvement \\
    530   & modules converted  &  0 & performance regressions
    %% TODO 530 / ???? how much of codebase?
  \end{tabular}
\end{table}

The Instagram web server has been running \SP{} in production since April 2021.
Overall, the results are very encouraging.
Instagram's internal profiling tools, which continuously monitor
performance, attribute a 3.7\% improvement in global CPU to \SP{} conversions.
\Cref{t:prod-stat} reports a few other production statistics.
Over 500 modules have been converted thus far.
None of the converted modules have gotten slower because of \SP{}


\subsection{Migration Path}

\SP{} first entered the Instagram web server as a replacement for Cython,
which Instagram had been using to speed up critical modules.
Cython performed well, but its partial adoption led to an awkward workflow
because unlike standard Python, Cython is a compiled language.
Replacing these modules with \SP{} let developers return to a conventional
Python workflow.
\SP{} (combined with Cinder) did not slow down any of these
performance-critical modules, and even sped a few up.
The net improvement was a 0.7\% reduction in global CPU usage.

Later \SP{} migrations have been led by profiling to find frequently-executed code.
The goal of these opportunistic migrations was to test whether \SP{} could
improve typical modules without slowing down their neighbors.
As a point of comparison, Typed Racket can improve individual Racket
modules but the type checks that it requires on module boundaries
can lead to huge slowdowns~\cite{tfgnvf-popl-2016,gtnffvf-jfp-2019}.
\SP{} does not appear to suffer in the same way.

During the first half of 2021, the \SP{} team identified critical
modules and proposed types to the maintainers of these modules.
Often, the types came with small code changes.
The accepted types and changes resulted in a 1\% improvement
in global CPU usage.
During the second half of 2021, the \SP{} team applied the same
process at a larger scale.
They also changed a code-generating module to output \SP{} code instead
of untyped Python code;
this one change brought over four hundred generated modules into the \SP{} fold.
Overall, these changes resulted in a 2\% global CPU improvement.

As of December 2021, the \SP{} team has converted 541 modules.
Most of these came from the tool (417); the rest
are from hand conversions (124).
These modules are well-mixed with untyped modules in the codebase.
All together, over thirty thousand identifiers cross a type boundary
at compile-time by either entering or leaving a \SP{} module.
Two-thirds of these crossings are exports from \SP{} modules
to untyped modules;
in other words, the typed identifiers are widely used throughout the Instagram web server.
%% data in src/static-python-imports-analysis
%% 2022-01-06 TODO gen-code is configuration system / access control ... what are stats omitting these?!

In a typical week, the team adds types to four additional modules.
These modules have seen speedups ranging from 0\% to 25\% depending
on their nature.
Regressions have not been an issue.
The only cause for alarm is when a module gets only a small improvement
without code changes.
%% - rough measure: 0 to 25% per module
%%   ... but depends on nature of module
%%   ... doesn't measure the cooperative effect of static-to-static cross-module calls
%% 2022-01-06 FILL team did significant work to fight regressions


\subsection{Analyzing Code Changes}
%% TODO boring unclear writing

Because the \SP{} team has been changing code as well as types
during its migrations, the question arises as to whether the code changes
raise any threats to validity.
It could be that the \SP{} approach to gradual typing only pays off
after significant modifications, or only for a restrictive style of programming.

To understand the code changes, we reviewed approximately 30 patches that
significantly improved performance.
In general, the patches have minor code changes and most of these changes
affect tests rather than production code.
Digging deeper, there are nine common kinds of changes.
Six of these address type errors; the other three focus on performance.

%% one significant change: replaced `contextlib.ContextDecorator` with `__static__.ContextDecorator`
%%  removes an extra call layer
%%  helps with (timing [ context managers / decorators ])

%% TODO section formatting looks good?
\subsubsection{Code Changes for the Type Checker}

    %% X bg: lookup the dino email about mocks
    %% And we do go through great lengths to make monkey patching work.  Because
    %% we need to enforce return types it ends up being not quite as flexible as
    %% normal Python.  For example one common pattern is to use Python’s mock
    %% library and we currently end up throwing lots of TypeError’s when a
    %% function which returns None is mocked and starts returning a MagicMock
    %% object instead.  But it’s an easy fix in that you can specify the return
    %% value to be compatible.  There may be more complex cases where you want to
    %% return a mock for an actual class.  For example in our code base there’s
    %% frequent mocking of the HttpRequest object and we have a MockReqeust object
    %% which is a subclass of HttpRequest so that these can still be mocked
    %% (luckily this actually pre-dated our static conversion work).

\begin{enumerate}
  \item
    \ipara{Fix type errors due to mock wrappers}
    When test code introduces a mock wrapper, it may change the return type of
    a function.
    For example, the wrapped version of a function that returns \code{None}
    will return a \code{MagicMock} object unless the programmer explicitly
    specifies a return value.
    %% Carl: This is an inherent and desired
    %% incompatibility; next half we plan to work on improving the Python mock
    %% framework to automatically create mocks of the correct types.

  \item
    \ipara{Change mocked function to expect positional arguments}
    \SP{} currently rewrites all function calls to use positional arguments.
    Mock-wrapped functions therefore need to positional arguments instead of keywords.
    %% Carl: we plan to fix this incompatibility by falling back to the original call
    %% arguments when we detect the target function has been patched.

  \item
    \ipara{Change functions to accept positional arguments}
    Functions that expect only keyword arguments cannot yet be called from \SP{} contexts.
    These functions need to change.
    %% Carl: We plan to support this next half.

  \item
    \ipara{Organize class and instance attributes}
    Whereas Python allows class attributes to serve as default values for instance attributes,
    \SP{} does not.
    Programmers have to decide where an attribute belongs so that \SP{} can optimize
    reads from instance attributes.
    %% Carl: (efficient via \code{LOAD\_FIELD})
    %% - related to github #37 ?

  \item
    \ipara{Move \code{@dataclass} and \code{Enum} classes}
    \SP{} does not support classes decorated with \code{@dataclass}
    and classes that are enums.
    Programmers have to move these classes to a different module for now.
    %% Carl: We have work in progress on supporting some Enum types, and plan to add a @dataclass intrinsic soon.

  \item
    \ipara{Remove unsupported Python features}
    Occurrences of the dynamic Python features listed in \cref{s:dynamic-python} must be rewritten or moved.
    According to the \SP{} team, these changes have not required any major rewrites.

\end{enumerate}


%% TODO section formatting looks good?
\subsubsection{Code Changes for Performance}

\begin{enumerate}
  \item
    \ipara{Convert \code{@classmethod} to \code{@staticmethod}}
    Static methods can get invoked directly as functions, bypassing the class.
    %% Carl: \code{INVOKE\_FUNCTION} instead of \code{INVOKE\_METHOD}

%% 2022-01-12
%% I don’t think it was a super common change, but there were some cases where we
%%     changed a @classmethod that didn’t actually use its cls argument to a
%%     @staticmethod. It looks like we have 24 static modules using a
%%     @staticmethod somewhere in the module, but I think in the majority of those
%%     there just happened to be some pre-existing staticmethods (they aren’t
%%     uncommon in our codebase); I think a minority were intentional
%%     optimizations on our part. And the pre-existing ones may or may not be
%%     meaningfully improving performance. So I’m not sure how valuable it is to
%%     report this metric.


  \item
    \ipara{Use primitive types}
    Any Python integers and booleans in hot code paths are better as
    \SP{} primitive values~(\cref{s:c-types}).
    %% - bg: focus on block, convert, no problem?

  \item
    \ipara{Use Checked Types}
    Occasionally, converting a Python dictionary or Python list to a
    checked type has a big impact on performance.
    As noted in \cref{s:checked-type}, these conversions affect all downstream
    clients of the data structure.
    %% - bg: how occasional? worried about power of the "gradual soundness" claim

\end{enumerate}


\subsubsection{Developer Experience}

For the most part, developers at Instagram have been happy adopting \SP{}.
The transition from Pyre to \SP{} has thus far delivered performance benefits
that outweigh the cost of code changes.

One significant pushback dealt with multiple inheritance.
The \SP{} team proposed a rewrite from multiple inheritance to single inheritance
to improve performance and static checks.
But the code maintainers preferred to keep multiple inheritance.
Because \SP{} defaults to Python behavior, the maintainers were able
to keep multiple inheritance alongside \SP{} code.

%% any more?


\subsection{Microbenchmarks}
%% https://github.com/facebookincubator/cinder/tree/cinder/3.8/Tools/benchmarks

%% TODO jit vs nojit

%% 2021-12-23: TODO decide whether to ask for typed benchmarks to report ratios
%%  Carl: we could commit to typed 2--3 more, but pystone etc (?!) are notoriously bad

%% Somehow though, we ought to have public code available.

Prior to deploying \SP{} in production, the team used microbenchmarks to study performance.
These microbenchmarks are admittedly small and quite different than typical application code.
That said, their use gave the team a coarse idea of whether \SP{} was on track during development.
We include these microbenchmarks to offer reproducible performance numbers;
unlike the Instagram web server, the benchmark code is public.

\Cref{t:microbenchmark} reports current microbenchmark results.
Each row gives the ratio of two running times: fully-typed \SP{} code versus
unannotated \SP{} code.
A ratio between 0 and 1 means that performance improved thanks to the types.
[FILL] numbers.

\begin{table}[t]
  \caption{Microbenchmark Performance Ratios}
  \label{t:microbenchmark}
  %% show untyped Python time?
  %% transpose?
  \begin{tabular}{ll}
    Name               & Typed / Dynamic \\\midrule
    \bmname{deltablue} &             0\% \\
    \bmname{fannkuch}  &             0\% \\
    \bmname{nbody}     &             0\% \\
    \bmname{nqueens}   &             0\% \\
    \bmname{pystone}   &             0\% \\
    \bmname{richards}  &         0.125\% \\
  \end{tabular}
\end{table}


%% TODO conclusions? threats?



\section{Related Work}
\label{s:related}

Nom~\cite{mt-oopsla-2017} and MonNom~\cite{mt-oopsla-2021} are foundational.
%% \footnote{In other words, \SP{} provides \emph{immediate accountability}~\cite{mt-oopsla-2017}.}

Full Monty~\cite{pmmwplck-oopsla-2013} core calculus for Python language, foundational for us.

Grace nominal transient aint bad at all

Thorn~\cite{wnlov-popl-2010} and StrongScript~\cite{rzv-ecoop-2015} are related.

PyPy is another runtime for Python
Pycket is built on PyPy and improves both Typed Racket~\cite{bbst-oopsla-2017}
and Reticulated Python~\cite{vsc-dls-2019}.

Type systems for Python ...
mypy pyre pytype;
Reticulated;

Optimizing type systems for Python ...
Reticulated;
mypyc;
Developer experience is one of the main advantages of \SP{} over mypyc,
which implements type-directed optimizations for CPython
by compiling source code to C extension modules.
\SP{} even used Cython at some point and found the mixed-workflow codebase painful.
%%  \item
%%    Prior to investing in \SP{}, Instagram used Cython to compile
%%    performance-critical code to C extension modules.
%%    Cython performed well, but mixing pure Python with a few compiled modules
%%    made for an awkward developer experience.
%%    \SP{} aims to match the Python workflow.

JS++ is a sound gradual type system for JavaScript~\cite{jspp}.
By default, it separates JS++ from JavaScript values:
every value with a type must be defined in JS++ code,
and every value that may have come from Javascript has
a special type \code{external} to mark it as such.
Programmers can manually cast external values to a
more precise type.
The casts may fail at runtime, and may create a copy of
the original value if they succeed (depends on the type).
%% 2022-01-09 how does JS++ protect arrays sent to JS?

Gradual soundness in \SP{} bears a strong resemblance to \emph{progressive types} vision
of a type system that comes in strict and lax forms~\cite{pqk-onward-2012}.
Primitive types are progressive types (and a little more) for base values.
Concrete types are essentially a progressive form of shallow types.


\section{Future Work}
\label{s:future}

%% Immediate future: fix adoption friction points
%% 2021-12-15: WHAT ARE THESE?
%% - mocks need manual return_value ... "next half we plan to work on improving the Python mock framework to automatically create mocks of the correct types."
%% - mock calls have to rewrite to expect positional args, not keywords "plan to avoid rewrite if fn is patched"
%% - functions have to expect positionals (can't do keyword only)
%% - @dataclass enum.Enum
%%
%% significant selection bias
%% all of this is the low-hanging fruit, haven't tackled uses of metaclasses even though looks promising
%%  because the pay / payoff isn't yet attractive


%% engineering: JIT profiling, instead of hand-requested JIT list
%% model: metaclasses

%% 2021-12-30 Whence Nominal GT?
%%
%% KC: I think in general adding gradual typing to Java/C# (an also Python) is boring:
%% - MonNom allows you to cast between an arbitrary type and dyn.
%% - dyn is effectively Object
%% - Upcasts, casts from an arbitrary type to Object, are already in the language
%% - Downcasts, casts from Object to an arbitrary type is also possible, but must be explicit in the fully static language
%% So MonNom effectively gives programmers another way to name Object, with which downcasts can be automatically inserted by the compiler.
%% I think this sentence covers all the interesting stuff about MonNom.
%%
%% SK: This matches my intuition that gradual nominal is just not very
%% interesting. KC puts this on a technical footing with his very astute comment,
%% which to me can be summarized as "every nominal language that has Object is
%% already gradual" [leaving aside the boring non-Object base type distinction].
       So the entire remaining game is one of making the programming interface
%% as painless as possible. Some of that is just a trivial matter of
%% syntactic defaults; some of it is making the type-checker more forgiving
%% and replacing static checks with dynamic checks. There doesn't seem to
%% be much else left.

Adapt SP ideas to a new setting, test performance takeaways.

Improve SP with generic types, structural types (lambda), \ldots.

Use confined GT to relax ChkDict ... let programmers decide whether
the type should reject or convert untyped data.
Maybe a ChkDict function could compile to an "overloaded" version with
fast and slow paths for ChkDict vs normal dict.

Build an automatic migration tool for Checked data.

Build a static analysis that hoists transient annotations to an early, shared point.
Take care to give quality error messages.

Optimize Python integer operations (in addition to the c types).
Hard because syntax like \code{a + b} may be the result of either \code{a.\_\_add\_\_(b)}
or \code{b.\_\_radd\_\_(a)} depending on runtime types and behavior.


\section{The Lessons}
\label{s:conclusion}

\SP{} is ambitious in many ways:
\begin{itemize}
  \item Type checker and compiler for Python syntax
  \item Fork of Python 3.8
  \item Add significant runtime components: vtable, classloader
  \item Add a method-based JIT
  \item Aiming for tens of thousands of typed modules (surely not all 45k in the server, but otoh the plan must be to hit other parts of IG too)
\end{itemize}

But \SP{} is also carefully targeted:
\begin{itemize}
  \item
    Not aiming for an exhaustive type checker.
    The availability of tools like Pyre already do a fine job.
    (Don't forget that consequently, it's helpful for undefined types go to dynamic.)
  \item
    Shallow types come first in the implementation, then concrete / checked
    (ditto for adoption)
  \item
    Not aiming for the full gradual guarantees.
    Only really matters for the initial migration, the common case.
    Indeed, it's important to not have the guarantee for primitives for their performance.
  \item
    The type checker is module-at-a-time, not fine-grained.
\end{itemize}



%% 2022-01-06 TODO multiple dimensions of gradual

\subsection{Nominal, Checked Types}
\label{s:concrete-migration}

CheckedDict enables strong type checks and optimizations, but is painful to use.
It is painful even though ChkDict supports the full dict API
(any context that uses a Python dict can use a ChkDict without code changes).

The problem is that the ChkDict type rejects all Python values.
It accepts only ChkDict values, built through a special constructor
that installs a tag for type tests and guards writes.
Suppose that \code{f} is an untyped dict function.
As is, it can process ChkDict but gets no benefit from type checks nor from optimizations.
Adding a ChkDict domain type enables optimizations within \code{f} but raises a non-local
problem: all callers of \code{f} must be sure to create a ChkDict.
These ChkDicts must also be monomorphic to match whatever type \code{f} chose.
It can take many edits to get a program running again after adding a ChkDict type.

Lessons:
\begin{itemize}
  \item
    Nominal types are not compatible with structual values, such as Python values.
    Programmers are forced to edit old code to use such types ... unless the recent Nom work has a better idea~\cite{mt-oopsla-2021}
  \item
    Monomorphic nominal types are even worse.
    Editing old code to fit their rigid constraints may not be feasible.

\end{itemize}

Enabling checked types has a huge ripple effect.
The worst case is changing an annotation to a expect a checked data structure:
\begin{enumerate}
  \item all callers must create a checked value by invoking the right constructor, and
  \item all clients of those callers can no longer expect an unchecked type.
\end{enumerate}
Programmers have to trace the annotation back to value-creation points, and then
ensure that all uses of those values are compatible.


\subsection{Pyre Compatibility}

Many decisions in \SP{} are influenced by the widespread use of Pyre at Instagram.
\begin{itemize}
  \item
    To minimize changes needed to adapt Pyre-annotated programs
    to \SP{}, the syntax of \SP{} is compatible with Pyre.
    Pyre can run useful checks on every \SP{} program.
  \item
    Because Pyre is a mature type checker, the \SP{} team has taken a
    depth-first approach to development.
    The language implements a small set of sound types efficiently and ignores
    other types.
    Of course, programmers who use advanced Pyre types can still use Pyre to
    get compile-time feedback.
  \item
    The fact that \SP{} interprets unknown Pyre types as type dynamic changes
    users' expectations about what the dynamic type should mean.
    It is not necessarily a label for untyped Python code; dynamic code
    can be analyzed to catch static bugs.
    %% FILL what are the checks?
\end{itemize}


\subsection{Conclusion}

Sound types in Static Python give programmers a way to improve performance.
The change is not forced, not intrusive;
programmers can keep using unsound types when working toward a deadline.

Conjecture that soundness will find bugs too, and lead to more reliable products.
Too soon to say.


\subsection{Communication}

How did we communicate effectively?
Starting point was GT survey [CITE].
Moved forward with example programs, see emails and github issues.
Our formalism had to cover their unit tests.
FILL

%\acks{
%  Thanks to
%  Guido van Rossum for stimulating tweets.
%  This work was partly supported by the US National Science Foundation.
%  This research was also developed with funding from the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL).
%  The views, opinions and/or findings expressed are those of the author and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S.~Government.
%  Greenman received support from NSF grant 2030859 to the CRA for the \href{https://cifellows2020.org}{CIFellows} project.
%}


{\sloppy
\printbibliography
}

\appendix

\section{GitHub Issues}
\label{a:github-issues}

FILL if we're going to list issues, might as well report the titles, labels, and open/closed status.

The 25 GitHub issues that we filed during the formalization effort may be found at the following three links:

\begin{itemize}
  \item \shorturl{https://}{github.com/facebookincubator/cinder/issues/created\_by/LuKC1024} (N=20)
  \item \shorturl{https://}{github.com/facebookincubator/cinder/issues/created\_by/bennn} (N=4)
  \item \shorturl{https://}{github.com/facebookincubator/cinder/issues/64} (N=1)
  %% extra, can't remember if we inspired Dino or not: https://github.com/facebookincubator/cinder/issues/32
\end{itemize}


\section{Pyre Limitations}

FILL condense the email thread in comment (see source)

% Pyre limitations
% 4 messages
% Ben Greenman <benjaminlgreenman@gmail.com>	Fri, Sep 24, 2021 at 11:18 AM
% To: Carl Meyer <carljm@fb.com>
% Cc: Dino Viehland <dinoviehland@fb.com>, "Lu, Kuang-Chen" <kuang-chen_lu@brown.edu>, Shriram Krishnamurthi <shriram@gmail.com>
% >>> When we “add types” to a function (by adopting its module into Static
% >>> Python, meaning Static Python now cares about the type annotations on that
% >>> function), there may be other code calling that function with types that
% >>> don’t match what it is annotated to receive. This then causes new runtime
% >>> type errors for those calls once the module is made static. So sometimes we
% >>> do have to fix those – we consider that to be fixing pre-existing type
% >>> errors that Pyre missed. We probably see a lot less of that because the
% >>> codebase is already type annotated and statically checked by Pyre, than we
% >>> would if we were adopting Static Python into a previously-not-type-checked
% >>> codebase.
% >>
% >> I'm surprised that you see any errors like this. Is Pyre unsound at
% >> these pre-existing spots?
% >
% > Yes, Pyre is unsound in various ways (as a purely static checker of a
% > gradually typed language must inherently be):
% >
% > 1. Some code just isn’t covered or checked by Pyre, because Pyre’s gradual
% > adoption strategy is to only check functions that have annotations in their
% > signature, or in extreme cases because the “code” that’s calling into Python
% > isn’t Python code at all, but rather Python C extension code or Cython code.
% > So Pyre has no visibility into calls from code it isn’t even checking.
% > 2. Any code with annotations involving the `Any` type (or missing
% > annotations) will have values that Pyre types as `Any`, and Pyre will
% > silently allow you to do anything with those values. (It has no other
% > choice, this is the only way that static-only gradual typing can work and
% > actually be gradual.) So anything typed as `Any` by Pyre is a potential
% > unsoundness. And although I say our codebase is largely type annotated,
% > given that lots of it is pre-typing code that can’t be accurately statically
% > annotated without rewriting it, there are plenty of Any annotations
% > (especially `Dict[str, Any]` annotations) floating around.
% > 3. People can (and do) just silence Pyre by adding a `# pyre-fixme` comment
% > that silences a type error on the following line, if they are doing some
% > mass adoption of types or otherwise just don’t feel up to fixing a type
% > error for whatever reason. This tends to happen most often in cases where an
% > annotation is just overly restrictive (e.g. claims to require `str` but in
% > fact the method handles `None` just fine, and the argument really should be
% > annotated as `str | None`.) So we have to fix the annotation to actually
% > match the runtime behavior, and then we also are able to remove the
% > pyre-fixme comments; everyone wins :)
% > 4. If the entire codebase and all its type annotations and type stubs agree
% > to lie about some dynamic wrinkle, Pyre will be satisfied that everything is
% > good (and will have no way to know otherwise). A particular case of this we
% > hit recently is that lots of values annotated as `str` in our codebase can
% > actually at runtime be `Promise` objects that dynamically behave like a
% > `str` if you treat them as one, and implement lazy translation of that
% > string to the current user’s language. So as we adopt such code into Static
% > Python, we have to correct those annotations from `str` to `str | Promise`
% > and handle the fallout. A similar case is the `weakref` module in the Python
% > standard library, which allows you to create “weak references” to objects
% > that will not keep them alive in the garbage collector, but will just
% > resolve to None if the object dies. There’s a `weakref.proxy` that creates a
% > transparent weak-reference proxy to any object, and sometimes methods will
% > return one of those but be annotated as returning the proxied type. This is
% > fine for Pyre, as long as the proxy is sufficiently transparent at the level
% > of Python semantics, but it doesn’t work for Static Python.
% 
% Points 1, 2, and 3 make sense.
% 
% I'm wondering if there are ways to write a fully-typed Pyre program (that
% doesn't use those escape hatches) but is still unsound.* It sounds like
% point 4 might be on this track ... but also that unchecked code (points 1,2)
% might be involved.
% 
% For the weakrefs case, would Pyre approve a function like this one:
% 
% ```
%   def f()->C:
%     return weakref.proxy(C())
% ```
% 
% If so, is that because Pyre comes with a builtin annotation for
% `weakrefs.proxy` that lets it return the Any type?
% 
% 
% * For comparison, TypeScript is unsound by design in a few ways. Any computed
% string can index any object. And all object properties are covariant, despite
% being mutable. I wonder if Pyre has similar "static unsoundness" by design.
% Lu, Kuang-Chen <kuang-chen_lu@brown.edu>	Fri, Sep 24, 2021 at 11:50 AM
% To: Ben Greenman <benjaminlgreenman@gmail.com>
% Cc: Shriram Krishnamurthi <shriram@gmail.com>
% (Internal discussion)
% 
% I think point 4, especially the `str` example, comes from the lack of structural typing. Interfaces can partially solve this problem. But with Nom-style interfaces, they have to define an interface and annotate the classes involved.
% [Quoted text hidden]
% Carl Meyer <carljm@fb.com>	Fri, Sep 24, 2021 at 12:37 PM
% To: Ben Greenman <benjaminlgreenman@gmail.com>
% Cc: Dino Viehland <dinoviehland@fb.com>, "Lu, Kuang-Chen" <kuang-chen_lu@brown.edu>, Shriram Krishnamurthi <shriram@gmail.com>
% [Quoted text hidden]
% I think it could be reasonable to define “fully-typed” as to inherently exclude all four points above! This would also exclude most real-world Python programs, but might still be an interesting definition, because even then there are some unsoundnesses remaining (that I didn’t cover above because they are not usually a problem for us in practice; in most cases I think because they involve parts of the type system we don’t even pretend to support yet in Static Python.) I’ll list some of these known unsoundnesses within the type system below.
% 
% In practice the Python language and standard library include features that are considered too painful to use if fully typed, generally because sound typing would require typing things as `object` (the top type), requiring `isinstance` checks everywhere to keep the static checker happy, so an unsound `Any` is preferred for usability reasons. Weakref proxies are just one example of this; there are many others.
% 
% https://github.com/python/typeshed is the shared repository of type stubs for the Python standard library and popular third-party packages (used by all the static checkers: Pyre, mypy, pyright, pytype…). You can search it for `Any` to find many many more examples.
% 
% 
% > For the weakrefs case, would Pyre approve a function like this one:
% >
% > ```
% >  def f()->C:
% >    return weakref.proxy(C())
% > ```
% >
% > If so, is that because Pyre comes with a builtin annotation for
% > `weakrefs.proxy` that lets it return the Any type?
% 
% Yes, exactly, this is in the shared “typeshed” repository: https://github.com/python/typeshed/blob/master/stdlib/_weakref.pyi#L33
% 
% >
% > * For comparison, TypeScript is unsound by design in a few ways. Any computed
% > string can index any object. And all object properties are covariant, despite
% > being mutable. I wonder if Pyre has similar "static unsoundness" by design.
% 
% Right, and the Python type system has some similar unsound-by-design choices. A few that I’m aware of:
% 
% - Mutable attributes ought to be invariant (because they are in effect both getters and setters, thus appearing in both covariant and contravariant position), thus subclasses should not be allowed to change the type of a base class attribute. But in practice all Python type checkers permit a subclass to narrow the type of a base class attribute. Now that `Final[]` annotations are available on attributes (which would make them soundly covariant), it might be more reasonable to close this hole, here’s discussion on the mypy bug tracker: https://github.com/python/mypy/issues/3208. We are currently sound here in Static Python, but in an odd way; we should error on the subclass attribute type override itself, but instead we ignore it and continue to use the base class attribute type for the subclass also.
% 
% - There are some issues with `typing.Type[]` types (these are the types of class objects themselves) and the fact that they are covariant (which is widely relied on.) This is unsound because incompatible constructor signature overriding is allowed in subclasses (which is also widely used). It’s also unsound because the type of the `self` argument to any overridden method will be Parent on the parent method and Child on the child method, which is the opposite of the normal requirement that method arguments (appearing in a contravariant position) must only be widened on subclass overrides. This means that if some function accepts a type and calls an unbound method on it, providing the `self` arg explicitly, the covariance of `Type` is unsound. On these we are currently OK only because we don’t yet support `Type[]` annotations.
% 
% - Type checking argument splatting in calls is unsound. If we have a dynamic list `l` or dict `d` and we have a call like `foo(*l)` or `foo(**d)`, unless foo accepts `(*args, **kwds)` it is impossible to statically type check the call. Making this sound would have to either require the callee to accept (*args, **kwds) or require that the list or dict is a statically known literal; the latter totally defeats the purpose of argument splatting, the former is more conceivable but still makes it a lot less useful, so in practice type checkers just allow these calls. This one is OK in Static Python because we simply fallback to dynamic calls (and thus runtime argument type checks in the callee) for these calls (I mean it’s OK in that it doesn’t result in cascading unsoundness — it still means runtime error rather than static one for the call itself.)
% 
% There are more cases than just these, but I think the others are more esoteric.
% 
% Carl


\end{document}
